{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Solution\n",
    "\n",
    "##### Author: Alex Sherman | alsherman@deloitte.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. print all the distinct entities tagged with 'CARDINAL'\n",
    "2. print all the distinct entities tagged with 'PERSON'\n",
    "3. print all the distinct entities tagged with 'GPE'\n",
    "\n",
    "For all exercises:\n",
    "- use a batch size of 100\n",
    "- disable the parse and tagger (ner is needed to add the tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seven\n",
      "at least eight\n",
      "one\n",
      "three\n",
      "at least three\n",
      "nine\n",
      "five\n",
      "three\n",
      "two\n",
      "at least two\n",
      "one\n",
      "two\n",
      "two\n",
      "one\n",
      "seven\n",
      "six\n",
      "two\n",
      "nine\n",
      "two\n",
      "two\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# print all the distinct entities tagged as a CARDINAL\n",
    "# search in immune_df.head(200)\n",
    "\n",
    "for doc in nlp.pipe(immune_df.head(200), batch_size=100, disable=['parser','tagger']):\n",
    "    for ent in doc.ents:\n",
    "        if 'CARDINAL' in ent.label_:\n",
    "            print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conley\n",
      "anti kell\n",
      "Wall time: 2.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# print all the distinct entities tagged as an organization (ORG)\n",
    "# search in immune_df.head(500)\n",
    "for doc in nlp.pipe(immune_df.head(500), batch_size=100, disable=['parser','tagger']):\n",
    "    for ent in doc.ents:\n",
    "        if 'PERSON' in ent.label_:\n",
    "            print(ent.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mls\n",
      "sweden\n",
      "switzerland\n",
      "india\n",
      "Wall time: 3.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# print all the distinct entities tagged as a geopolitical entity (GPE)\n",
    "# search in immune_df.head(1000)\n",
    "for doc in nlp.pipe(immune_df.head(1000), batch_size=100, disable=['parser','tagger']):\n",
    "    for ent in doc.ents:\n",
    "        if 'GPE' in ent.label_:\n",
    "            print(ent.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. Count how many time each individual entity appears\n",
    "2. Create a mapping that keeps track of every combination of entities pairs that appear in the same sentence\n",
    "3. Count how many times each entity combo appears\n",
    "4. Print the entity combos (using sorted) in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 199 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create a defaultdict(int) called entity_relations\n",
    "entity_relations = defaultdict(int)\n",
    "\n",
    "# create an empty list called counter_entities \n",
    "counter_entities = []\n",
    "\n",
    "# during testing set .head() to a smaller number such as .head(1000) \n",
    "for doc in nlp.pipe(immune_df.head(1000), disable=['parser','tagger', 'ner']):\n",
    "\n",
    "    # store the token.text for all the tokens containing the letters 'toxic' (i.e. 'toxic' in term)\n",
    "    # use a list comprehension\n",
    "    entities = [token.text for token in doc if 'toxic' in token.text]\n",
    "\n",
    "    # add the tokens from the current doc to counter_entities (use += to add the token.text)\n",
    "    counter_entities += entities\n",
    "    \n",
    "    # create combinations of two terms each time multiple 'toxic' words appear\n",
    "    # increment the count in entity_relations defaultdict each time a combo is repeated\n",
    "    for combo in combinations(entities, 2):\n",
    "        entity_relations[combo] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'cytotoxic': 24, 'cytotoxicity': 3, 'bacteriotoxic': 1, 'toxicity': 1, 'toxic': 1, 'lymphocytotoxic': 1, 'thyrotoxic': 1, 'thyrotoxicosis': 1})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(counter_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('cytotoxic', 'cytotoxic'), 3),\n",
       " (('cytotoxic', 'cytotoxicity'), 1),\n",
       " (('thyrotoxic', 'thyrotoxicosis'), 1)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the entity pairs in descending order\n",
    "sorted(entity_relations.items(), key=lambda x: x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
