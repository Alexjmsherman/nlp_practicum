{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Classes\n",
    "\n",
    "In this notebook we will create a simplified version of the CountVectorizer class from scikit-learn as a means to explain Python classes. \n",
    "\n",
    "CountVectorizer converts a collection of text documents to a matrix of token counts. In this, it is a utility to vectorize unstructured text by counting the number of times that each token shows up in a corpora of documents. \n",
    "- CountVectorizer Code: https://github.com/scikit-learn/scikit-learn/blob/f3320a6f/sklearn/feature_extraction/text.py#L541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of documents\n",
    "text = [\n",
    "      'This is the first document'\n",
    "    , 'This is the second second document'\n",
    "    , 'And the third one'\n",
    "    , 'Is it the first document again'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe scikit-learn CountVectorizer to determine what we need to recreate\n",
    "\n",
    "- CountVectorizer acts as a blueprint to create different versions of CountVectorizer\n",
    "- We start by creating one instance of CountVectorizer, which we have decided to call vect, using the default parameters\n",
    "- Let's observe some of the available attributes and methods (terminology we will discuss later) in CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an instance of countvectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "# when we print vect, we see its hyperparameters\n",
    "print(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['again',\n",
       " 'and',\n",
       " 'document',\n",
       " 'first',\n",
       " 'is',\n",
       " 'it',\n",
       " 'one',\n",
       " 'second',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The vectorizer learns the vocabulary when we fit it with our documents. \n",
    "# This means it learns the distinct tokens (terms) in the text of the documents. \n",
    "# We can observe these with the method get_feature_names\n",
    "\n",
    "vect.fit(text)\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform creates a sparse matrix, identifying the indices where terms are stores in each document\n",
    "# This sparse matrix has 4 rows and 9 columns\n",
    "\n",
    "vect.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is easier to understand when we covert the sparse matrix into a dense matrix or pandas DataFrame\n",
    "vect.transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>again</th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   again  and  document  first  is  it  one  second  the  third  this\n",
       "0      0    0         1      1   1   0    0       0    1      0     1\n",
       "1      0    0         1      0   1   0    0       2    1      0     1\n",
       "2      0    1         0      0   0   0    1       0    1      1     0\n",
       "3      1    0         1      1   1   1    0       0    1      0     0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = vect.transform(text).toarray()\n",
    "columns = vect.get_feature_names()\n",
    "pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And',\n",
       " 'Is',\n",
       " 'This',\n",
       " 'again',\n",
       " 'document',\n",
       " 'first',\n",
       " 'is',\n",
       " 'it',\n",
       " 'one',\n",
       " 'second',\n",
       " 'the',\n",
       " 'third']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by instantiating CountVectorizer with differnt parameters, we can change the vocabulary\n",
    "# lowercase determines if all words should be lowercase, setting it to False includes uppercase words\n",
    "\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "vect.fit(text)\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['document', 'second']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stops words determine if we should include common words (e.g. and, is, the) which show up in most documents\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "vect.fit(text)\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['document', 'first', 'is', 'second', 'the']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max features determines the maximum number of features to display\n",
    "vect = CountVectorizer(max_features=5)\n",
    "vect.fit(text)\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions to replicate CountVectorizer\n",
    "\n",
    "#### The are many other methods and parameters, but lets begin with the following few:\n",
    "\n",
    "#### Methods\n",
    "- fit\n",
    "- get_feature_names\n",
    "- transform\n",
    "\n",
    "#### Attributes\n",
    "- lowercase\n",
    "- stop_words\n",
    "- max_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a fit function, to recreate the fit functionality from CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'Is', 'This', 'again', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third']\n"
     ]
    }
   ],
   "source": [
    "def fit(raw_documents):\n",
    "    \"\"\"\n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    \n",
    "    :return sorted_tokens: tokens sorted\n",
    "    \"\"\"\n",
    "    \n",
    "    # combine all of the raw_documents into a string, separated by spaces\n",
    "    combined_sentences = ' '.join(raw_documents)\n",
    "    \n",
    "    # separate the string into individual tokens (terms), split the overall string by spaces \n",
    "    all_tokens = combined_sentences.split(' ')\n",
    "    \n",
    "    # only keep the set of distinct tokens (i.e. do not keep multiple copies of a word)\n",
    "    distinct_tokens = set(all_tokens) \n",
    "    \n",
    "    # sort the terms alphabetically\n",
    "    sorted_tokens = sorted(list(distinct_tokens))\n",
    "\n",
    "    return sorted_tokens\n",
    "\n",
    "sorted_tokens = fit(text)\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a lowercase parameter in fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['again', 'and', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "def fit(raw_documents, lowercase=False):\n",
    "    \"\"\"\n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    :param lowercase: boolean, default=True\n",
    "        Convert all characters to lowercase before tokenizing\n",
    "    \n",
    "    :return sorted_tokens: tokens sorted\n",
    "    \"\"\"\n",
    "    \n",
    "    # add a check for the lowercase parameter\n",
    "    # convert all documents to lowercase\n",
    "    if lowercase:\n",
    "        raw_documents = [doc.lower() for doc in raw_documents]\n",
    "\n",
    "    combined_sentences = ' '.join(raw_documents)\n",
    "    all_tokens = combined_sentences.split(' ')\n",
    "    distinct_tokens = set(all_tokens) \n",
    "    sorted_tokens = sorted(list(distinct_tokens))\n",
    "\n",
    "    return sorted_tokens\n",
    "\n",
    "sorted_tokens = fit(text, lowercase=True)\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original documents: ['This is the first document', 'This is the second second document', 'And the third one', 'Is it the first document again'] \n",
      "\n",
      "lowercase documents: ['this is the first document', 'this is the second second document', 'and the third one', 'is it the first document again']\n"
     ]
    }
   ],
   "source": [
    "# observe the new list comprehension to lowercase the original documents\n",
    "print('Original documents: {} \\n'.format(text))\n",
    "\n",
    "print('lowercase documents: {}'.format([doc.lower() for doc in text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a stop_words parameter in fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'Is', 'This', 'again', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third']\n"
     ]
    }
   ],
   "source": [
    "def fit(raw_documents, stop_words):\n",
    "    \"\"\" \n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    :param stop_words: string {'english'} or list\n",
    "        If 'english', a built-in stop word list for English is used.\n",
    "        If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    \n",
    "    :return sorted_tokens: tokens sorted by frequency\n",
    "    \"\"\"\n",
    "\n",
    "    combined_sentences = ' '.join(raw_documents)\n",
    "    all_tokens = combined_sentences.split(' ')\n",
    "    \n",
    "    # add a check for the stop_words parameter\n",
    "    # remove all words that are in the stops_words list\n",
    "    # otherwise keep all distinct tokens\n",
    "    if stop_words:\n",
    "        distinct_tokens = [token for token in set(all_tokens) \n",
    "                           if token not in stop_words]\n",
    "    else:\n",
    "        distinct_tokens = set(all_tokens)\n",
    "    \n",
    "    sorted_tokens = sorted(list(distinct_tokens))\n",
    "\n",
    "    return sorted_tokens\n",
    "\n",
    "sorted_tokens = fit(text, stop_words=['a','of'])\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'Is', 'This', 'again', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third']\n"
     ]
    }
   ],
   "source": [
    "# stop_words often include many tokens. Typing them each time leaves room for error.\n",
    "print(fit(text, stop_words=['A','Another']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would get tedious to add in a list of stops words ourselves every time we use the fit function. Also we may forget to add the same words every time or miss a couple words in a long list.\n",
    "\n",
    "To avoid this we can add a static list of stop words inside of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'Is', 'This', 'again', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third']\n"
     ]
    }
   ],
   "source": [
    "# add a default list of stop words\n",
    "\n",
    "def fit(raw_documents, stop_words=None):\n",
    "    \"\"\"\n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    :param stop_words: string {'english'} or list\n",
    "        If 'english', a built-in stop word list for English is used.\n",
    "        If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    \n",
    "    :return sorted_tokens: tokens sorted\n",
    "    \"\"\"\n",
    "        \n",
    "    combined_sentences = ' '.join(raw_documents)\n",
    "    all_tokens = combined_sentences.split(' ')\n",
    "    \n",
    "    # add a check if you add your own stop words\n",
    "    if stop_words == 'english':\n",
    "        # include a default list of stop words in the function\n",
    "        stop_words = ['a','of']\n",
    "    \n",
    "    if stop_words:\n",
    "        distinct_tokens = [token for token in set(all_tokens) \n",
    "                           if token not in stop_words]\n",
    "    else:\n",
    "        distinct_tokens = set(all_tokens)\n",
    "\n",
    "    sorted_tokens = sorted(list(distinct_tokens))\n",
    "\n",
    "    return sorted_tokens\n",
    "\n",
    "sorted_tokens = fit(text, stop_words=None)\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement max features paramater to fit\n",
    "\n",
    "- max features return the top most common terms; thus, we need to determine the word count for each token\n",
    "- We could accomplish this in the fit function, but it makes sense to split it into its own function as we may need to use the output in multiple places (e.g. use token_stats for fit and transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('m', 1), ('i', 4), ('s', 4), ('p', 2)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "s = 'mississippi'\n",
    "\n",
    "d = defaultdict(int)\n",
    "for k in s:\n",
    "    d[k] += 1\n",
    "\n",
    "d.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will be adding many unseen terms to a dict and counting their frequency across documents, defaultdict is useful.\n",
    "\n",
    "defaultdict adds to dict functionality, allowing us to simplify the code by avoiding a check if an instance has been added to a dict before incrementing its count.\n",
    "\n",
    "- defaultdict documentation: https://docs.python.org/2/library/collections.html#collections.defaultdict\n",
    "- defaultdict examples: https://www.accelebrate.com/blog/using-defaultdict-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'document', 'This', 'is', 'first', 'second', 'And', 'third', 'one', 'Is', 'it', 'again']\n"
     ]
    }
   ],
   "source": [
    "def get_token_stats(raw_documents):\n",
    "    \"\"\" \n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    \n",
    "    :return frequent_tokens: list tokens sorted by frequency\n",
    "    \"\"\"\n",
    "\n",
    "    # set an empty dict to store each token and its count e.g. {'token1': 1, 'token2': 2}\n",
    "    token_stats = defaultdict(int)\n",
    "    \n",
    "    # iterate through all documents, then iterate through all terms\n",
    "    # increase the count for a token by one each time it appears in any document\n",
    "    for doc in raw_documents:\n",
    "        for term in doc.split(' '):\n",
    "            token_stats[term] += 1\n",
    "    \n",
    "    # create a list of the tokens sorted by occurance (most frequent first)\n",
    "    frequent_tokens_with_count = sorted(\n",
    "        token_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "    frequent_tokens = [token[0] for token in frequent_tokens_with_count]\n",
    "\n",
    "    return frequent_tokens\n",
    "\n",
    "frequent_tokens = get_token_stats(text)\n",
    "print(frequent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the lowercase functionality to the get_token_stats function to avoid including the same word multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'is', 'document', 'this', 'first', 'second', 'and', 'third', 'one', 'it', 'again']\n"
     ]
    }
   ],
   "source": [
    "def get_token_stats(raw_documents, lowercase=True):\n",
    "    \"\"\" \n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    :param lowercase: boolean, default=True\n",
    "        Convert all characters to lowercase before tokenizing\n",
    "    \n",
    "    :return token_stats: dict of {token:count} \n",
    "    :return frequent_tokens: list tokens sorted by frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    # add a check for the lowercase parmeter\n",
    "    # use the same functionality as in fit\n",
    "    if lowercase:\n",
    "        raw_documents = [doc.lower() for doc in raw_documents]\n",
    "\n",
    "    token_stats = defaultdict(int)\n",
    "    for doc in raw_documents:\n",
    "        for term in doc.split(' '):\n",
    "            token_stats[term] += 1\n",
    "\n",
    "    # create a list of the tokens sorted by occurance (most frequent first)\n",
    "    frequent_tokens_with_count = sorted(\n",
    "        token_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "    frequent_tokens = [token[0] for token in frequent_tokens_with_count]\n",
    "\n",
    "    return frequent_tokens\n",
    "\n",
    "frequent_tokens = get_token_stats(text)\n",
    "print(frequent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete max features paramater in fit, using get_token_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'is', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "def fit(raw_documents, lowercase=True, max_features=None, frequent_tokens=None):\n",
    "    \"\"\"\n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    :param max_features: int or None, default=None\n",
    "        If not None, build a vocabulary that only consider the top\n",
    "        max_features ordered by term frequency across the corpus\n",
    "    :param frequent_tokens: tokens ordered by frequency in the raw_documents\n",
    "        Tokens with the same frequency are in a random order, accordingly\n",
    "        tokens that display only once are not ordered alphabetically\n",
    "    \n",
    "    :return sorted_tokens: tokens sorted by frequency\n",
    "    \"\"\"\n",
    "\n",
    "    if lowercase:\n",
    "        raw_documents = [doc.lower() for doc in raw_documents]\n",
    "\n",
    "    combined_sentences = ' '.join(raw_documents)\n",
    "    all_tokens = combined_sentences.split(' ')\n",
    "    distinct_tokens = set(all_tokens)\n",
    "\n",
    "    # add max_features parameter check\n",
    "    if max_features:\n",
    "        # use the sorted list of tokens; filter to the # of max_features\n",
    "        tokens_to_keep = frequent_tokens[0: max_features]\n",
    "        # only keep tokens that are in the tokens_to_keep list\n",
    "        distinct_tokens = [token for token in distinct_tokens if token in tokens_to_keep]\n",
    "\n",
    "    sorted_tokens = sorted(list(distinct_tokens))\n",
    "\n",
    "    return sorted_tokens\n",
    "\n",
    "sorted_tokens = fit(text, max_features=5, frequent_tokens=frequent_tokens)\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unintended results from passing the same data to related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now begining to pass the same data from one function to another. Here we create the frequent_tokens variable in get_token_stats, then we pass it to the fit function.\n",
    "\n",
    "Both functions work on the same raw_documents; thus, if we preprocess the documents differently, we may get unintended results. \n",
    "\n",
    "As an example of this, change the lowercase parameter value to 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'is', 'the']\n"
     ]
    }
   ],
   "source": [
    "sorted_tokens = fit(text, lowercase=False, max_features=5, frequent_tokens=frequent_tokens)\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer get five max_features when lowercase is 'False'\n",
    "\n",
    "We converted all documents to lowercase in get_token_stats, but have not done the same yet in fit. Therefore, even though we are asking for five max_features only four are returned. In this case, the missing term is 'This' which is always uppercase in the original documents (e.g. 'This is the first document')\n",
    "\n",
    "Some of the consequences are that we must recreate the same code in multiple places (add a lowercase check in both get_token_stats and fit) and we must remember all the parameters we used previously to avoid analyzing different text in each sequence of functions.\n",
    "\n",
    "Soon we will observe how classes alleviate issues like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'second', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "# put all the parameters together\n",
    "\n",
    "def fit(raw_documents, lowercase=True, stop_words=None, max_features=None, frequent_tokens=None):\n",
    "    \"\"\"\n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    :param lowercase: boolean, default=True\n",
    "        Convert all characters to lowercase before tokenizing\n",
    "    :param stop_words: string {'english'} or list\n",
    "        If 'english', a built-in stop word list for English is used.\n",
    "        If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    :param max_features: int or None, default=None\n",
    "        If not None, build a vocabulary that only consider the top\n",
    "        max_features ordered by term frequency across the corpus\n",
    "    :param frequent_tokens: tokens ordered by frequency in the raw_documents\n",
    "        Tokens with the same frequency are in a random order, accordingly\n",
    "        tokens that display only once are not ordered alphabetically\n",
    "        \n",
    "    :return sorted_tokens: tokens sorted by frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    if lowercase:\n",
    "        raw_documents = [doc.lower() for doc in raw_documents]\n",
    "\n",
    "    combined_sentences = ' '.join(raw_documents)\n",
    "    all_tokens = combined_sentences.split(' ')\n",
    "\n",
    "    if stop_words == 'english':\n",
    "        stop_words = ['a','of']\n",
    "    \n",
    "    if stop_words:\n",
    "        distinct_tokens = [token for token in set(all_tokens) if token not in stop_words]\n",
    "        # remove stops words from frequent_tokens\n",
    "        frequent_tokens = [token for token in frequent_tokens if token not in stop_words]\n",
    "    else:\n",
    "        distinct_tokens = set(all_tokens)\n",
    "        \n",
    "    if max_features:\n",
    "        tokens_to_keep = frequent_tokens[0: max_features]\n",
    "        distinct_tokens = [token for token in distinct_tokens if token in tokens_to_keep]\n",
    "\n",
    "    sorted_tokens = sorted(list(distinct_tokens))\n",
    "\n",
    "    return sorted_tokens\n",
    "\n",
    "sorted_tokens = fit(\n",
    "    text, stop_words=['is','first'], max_features=5, frequent_tokens=frequent_tokens)\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform in scikit-learn returns a sparse matrix. The matrix includes the indices and count of each token in the raw documents. We have already created this in get_token_stats.\n",
    "\n",
    "Let's update get_token_stats to avoid recalculating the token counts in transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'this': 2, 'is': 3, 'the': 4, 'first': 2, 'document': 3, 'second': 2, 'and': 1, 'third': 1, 'one': 1, 'it': 1, 'again': 1})\n"
     ]
    }
   ],
   "source": [
    "def get_token_stats(raw_documents, lowercase=True):\n",
    "    \"\"\" \n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    :param lowercase: boolean, default=True\n",
    "        Convert all characters to lowercase before tokenizing\n",
    "    \n",
    "    :return token_stats: dict of {token:count} \n",
    "    :return frequent_tokens: list tokens sorted by frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    if lowercase:\n",
    "        raw_documents = [doc.lower() for doc in raw_documents]\n",
    "\n",
    "    token_stats = defaultdict(int)\n",
    "    for doc in raw_documents:\n",
    "        for term in doc.split(' '):\n",
    "            token_stats[term] += 1\n",
    "\n",
    "    frequent_tokens_with_count = sorted(\n",
    "        token_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "    frequent_tokens = [token[0] for token in frequent_tokens_with_count]\n",
    "\n",
    "    # return both the token_stats and frequent_tokens\n",
    "    return token_stats, frequent_tokens\n",
    "\n",
    "# update tuple unpacking of token_stats and frequent_tokens\n",
    "token_stats, frequent_tokens = get_token_stats(text)\n",
    "print(token_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3, 4), (0, 1, 3), (1, 3, 4), (1, 2, 2), (1, 2, 2), (1, 1, 3), (2, 3, 4), (3, 3, 4), (3, 1, 3)]\n"
     ]
    }
   ],
   "source": [
    "def transform(raw_documents, sorted_tokens, token_stats):\n",
    "    \"\"\"\n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    :param sorted_tokens: tokens sorted by frequency\n",
    "    :param token_stats: tuple of (token:count)\n",
    "    \n",
    "    :return sparse_matrix: list of tuples with replicating a sparse matrix\n",
    "        indicates the index of non-zero tokens in a dense matrix: (row_num, column_num, count)\n",
    "    \"\"\"\n",
    "\n",
    "    # set a container for the sparse matrix output\n",
    "    # expected output: (row_num, column_num, count)\n",
    "    sparse_matrix = []\n",
    "\n",
    "    # create a dict of the key:value pairs of the \n",
    "    # {token:index} for each token \n",
    "    # each token is a column in a dense matrix\n",
    "    tokens_col_index = {token:ind for ind, token in enumerate(sorted_tokens)}\n",
    "\n",
    "    # enumerate each raw_document to get the row number \n",
    "    # (index of document in list) \n",
    "    for row_num, doc in enumerate(raw_documents):\n",
    "        # iterate through all tokens in the doc\n",
    "        for token in doc.split(' '):\n",
    "            # only retain selected tokens from fit\n",
    "            if token in sorted_tokens:\n",
    "                # (row_num, column_num, count)\n",
    "                sparse_matrix.append((row_num, tokens_col_index[token], token_stats[token]))\n",
    "    \n",
    "    return sparse_matrix\n",
    "\n",
    "sparse_matrix = transform(text, sorted_tokens=sorted_tokens, token_stats=token_stats)\n",
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have the implemented fit and transform, we run the entire process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS: \n",
      "['document', 'first', 'is', 'the', 'this'] \n",
      "\n",
      "DATA: \n",
      "[(0, 4, 3), (0, 8, 4), (0, 3, 2), (0, 2, 3), (1, 4, 3), (1, 8, 4), (1, 7, 2), (1, 7, 2), (1, 2, 3), (2, 8, 4), (2, 9, 1), (2, 6, 1), (3, 5, 1), (3, 8, 4), (3, 3, 2), (3, 2, 3), (3, 0, 1)] \n",
      "\n",
      "TOKEN_STATS: \n",
      "defaultdict(<class 'int'>, {'this': 2, 'is': 3, 'the': 4, 'first': 2, 'document': 3, 'second': 2, 'and': 1, 'third': 1, 'one': 1, 'it': 1, 'again': 1}) \n",
      "\n",
      "frequent_tokens: \n",
      "['the', 'is', 'document', 'this', 'first', 'second', 'and', 'third', 'one', 'it', 'again']\n"
     ]
    }
   ],
   "source": [
    "token_stats, frequent_tokens = get_token_stats(raw_documents=text)\n",
    "\n",
    "sorted_tokens = fit(\n",
    "      raw_documents=text\n",
    "    , lowercase=True\n",
    "    , stop_words=None\n",
    "    , max_features=5\n",
    "    , frequent_tokens=frequent_tokens)\n",
    "\n",
    "sparse_matrix = transform(\n",
    "      raw_documents=text\n",
    "    , sorted_tokens=sorted_tokens\n",
    "    , token_stats=token_stats)\n",
    "\n",
    "print('COLUMNS: \\n{} \\n'.format(sorted_tokens))\n",
    "print('DATA: \\n{} \\n'.format(sparse_matrix))\n",
    "print('TOKEN_STATS: \\n{} \\n'.format(token_stats))\n",
    "print('frequent_tokens: \\n{}'.format(frequent_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inconveniences in the above approaches\n",
    "- The various functions are spread throughout the codebase\n",
    "- We have to keep giving new names to intermediary steps if we want to keep multiple versions (e.g. token_stats1, token_stats2)\n",
    "- If we run the code multiple times, it is difficult to remember the parameters we used\n",
    "- It is annoying to keep passing the same params into the multiple functions\n",
    "- We have to remember to run helper functions (e.g. get_token_stats) before we can run other functions, even though these should always occur\n",
    "- Multiple objects are returned from some functions, even though we may not always need both. We either have to call these functions (e.g get_token_stats) inside of multiple other functions (leading to duplicate computation) or bring them outside of the functions as be did above, complicating the code and usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we had a collection of related data and functions. A class acts as a container for these related data and functions.\n",
    "\n",
    "Think of a class as a blueprint that defines how to create objects. Classes use the following terminology:\n",
    "- Class: blueprint\n",
    "- Instance: a single object created from a class\n",
    "- Attributes: variables in a class\n",
    "- Methods: functions in a class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple class with one attribute\n",
    "\n",
    "class CountVectorizer:\n",
    "    \n",
    "    def __init__(self, lowercase=True):\n",
    "        self.lowercase = lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classes start with the word class\n",
    "- Class names use CamelCase\n",
    "- functions inside of a class (def) are called methods\n",
    "- variables inside of a class are called attributes (lowercase)\n",
    "- \\__init\\__ stands for initialization\n",
    "- the double underscores indicate that \\__init\\__ is a special or dunder (double underscore) method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self is how we refer to an instane of a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classes build an object that retains the data with which it is initialized. \n",
    "- Unlike a function, we can recall the parameters used to fit the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer()\n",
    "cv2 = CountVectorizer(lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.lowercase for csv1: True\n",
      "self.lowercase for csv2: False\n"
     ]
    }
   ],
   "source": [
    "print('self.lowercase for csv1: {}'.format(cv1.lowercase))\n",
    "print('self.lowercase for csv2: {}'.format(cv2.lowercase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes combine related data and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the fit function and add it to our CountVectorizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'Is', 'This', 'again', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third']\n"
     ]
    }
   ],
   "source": [
    "def fit(raw_documents):    \n",
    "    combined_sentences = ' '.join(raw_documents)\n",
    "    all_tokens = combined_sentences.split(' ')\n",
    "    distinct_tokens = set(all_tokens) \n",
    "    sorted_tokens = sorted(list(distinct_tokens))\n",
    "\n",
    "    return sorted_tokens\n",
    "\n",
    "sorted_tokens = fit(text)\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'Is', 'This', 'again', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third']\n"
     ]
    }
   ],
   "source": [
    "class CountVectorizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocabulary_ = None\n",
    "\n",
    "    def fit(self, raw_documents):\n",
    "        \"\"\"\n",
    "        :param raw_documents: iterable over raw text documents\n",
    "        \"\"\"\n",
    "\n",
    "        combined_sentences = ' '.join(raw_documents)\n",
    "        all_tokens = combined_sentences.split(' ')\n",
    "        distinct_tokens = set(all_tokens) \n",
    "        sorted_tokens = sorted(list(distinct_tokens))\n",
    "\n",
    "        self.vocabulary_ = sorted_tokens\n",
    "        \n",
    "        return self\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(text)\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we place the fit function inside of our CountVectorizer Class, we call it a method. The code for the fit is the same except that we no longer return the vocabulary, instead we store it as a permanent attribute in the class called vocabulary\\_.   We start by initializing vocabulary_ to None then populate it in the fit method. We will explore the impact of returning self later. \n",
    "\n",
    "- To use our class, we first create an instance of the class: cv = CountVectorizer()\n",
    "- Then we fit it with a corpora of documents: cv.fit(text)\n",
    "- By storing the distinct tokens (sorted\\_tokens from the previous function), we can call the tokens at any later point: cv.vocabulary_\n",
    "\n",
    "NOTE: Here we are using common syntax in scikit-learn where an underscore after an attribute name (e.g. vocabulary_) means the attribute only has data after running a method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.vocabulary_ for csv1: ['just', 'one', 'sentence']\n",
      "self.vocabulary_ for csv2:  ['a', 'different', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "cv1 = CountVectorizer()\n",
    "cv2 = CountVectorizer()\n",
    "\n",
    "cv1.fit(raw_documents=['just one sentence'])\n",
    "print('self.vocabulary_ for cv1: {}'.format(cv1.vocabulary_))\n",
    "\n",
    "cv2.fit(['a different sentence'])\n",
    "print('self.vocabulary_ for cv2:  {}'.format(cv2.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create multiple instances of the same class and each will remember there separate attributes. This way we only need to provide a different name for the overall instance, not every attribute if we want to store multiple CountVectorizer instances with different parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['again', 'and', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "def fit(raw_documents, lowercase=False):\n",
    "    \"\"\"\n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    :param lowercase: boolean, default=True\n",
    "        Convert all characters to lowercase before tokenizing\n",
    "    \n",
    "    :return sorted_tokens: tokens sorted by frequency\n",
    "    \"\"\"\n",
    "\n",
    "    if lowercase:\n",
    "        raw_documents = [doc.lower() for doc in raw_documents]\n",
    "\n",
    "    combined_sentences = ' '.join(raw_documents)\n",
    "    all_tokens = combined_sentences.split(' ')\n",
    "    distinct_tokens = set(all_tokens) \n",
    "    sorted_tokens = sorted(list(distinct_tokens))\n",
    "\n",
    "    return sorted_tokens\n",
    "\n",
    "sorted_tokens = fit(text, lowercase=True)\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['again', 'and', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "class CountVectorizer:\n",
    "\n",
    "    def __init__(self, lowercase=True):\n",
    "        self.lowercase = lowercase\n",
    "        self.vocabulary_ = None\n",
    "\n",
    "    def fit(self, raw_documents):\n",
    "        \"\"\"\n",
    "        :param raw_documents: iterable over raw text documents\n",
    "        \"\"\"\n",
    "        \n",
    "        # add a check for the lowercase parameter\n",
    "        if self.lowercase:\n",
    "            raw_documents = [doc.lower() for doc in raw_documents]\n",
    "\n",
    "        combined_sentences = ' '.join(raw_documents)\n",
    "        all_tokens = combined_sentences.split(' ')\n",
    "        distinct_tokens = set(all_tokens) \n",
    "        sorted_tokens = sorted(list(distinct_tokens))\n",
    "\n",
    "        self.vocabulary_ = sorted_tokens\n",
    "        \n",
    "        return self\n",
    "    \n",
    "cv = CountVectorizer()\n",
    "cv.fit(text)\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding self to the method signature, we can use all of the stored attributes in the class instance (i.e. in the \\__init\\__) without having to pass them in as parameters to the method. For instance, we use self.lowercase in the fit method, only by passing in self."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'Is', 'This', 'again', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third']\n"
     ]
    }
   ],
   "source": [
    "# add in a stop words parameter\n",
    "def fit(raw_documents, stop_words=None):\n",
    "    \"\"\"\n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    :param stop_words: string {'english'} or list\n",
    "        If 'english', a built-in stop word list for English is used.\n",
    "        If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    \n",
    "    :return sorted_tokens: tokens sorted by frequency\n",
    "    \"\"\"\n",
    "\n",
    "    combined_sentences = ' '.join(raw_documents)\n",
    "    all_tokens = combined_sentences.split(' ')\n",
    "    \n",
    "    # add a check if you add your own stop words\n",
    "    if stop_words == 'english':\n",
    "        # include a default list of stop words in the function\n",
    "        stop_words = ['a','of']\n",
    "    \n",
    "    if stop_words:\n",
    "        distinct_tokens = [token for token in set(all_tokens) if token not in stop_words]\n",
    "    else:\n",
    "        distinct_tokens = set(all_tokens)\n",
    "\n",
    "    sorted_tokens = sorted(list(distinct_tokens))\n",
    "\n",
    "    return sorted_tokens\n",
    "\n",
    "sorted_tokens = fit(text, stop_words=None)\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['again', 'and', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "class CountVectorizer:\n",
    "    # set a static list of stop words as part of the class\n",
    "    ENGLISH_STOP_WORDS = ['a', 'of', 'in', 'the', 'to']\n",
    "    \n",
    "    def __init__(self, lowercase=True, stop_words=None):\n",
    "        self.lowercase = lowercase\n",
    "        self.stop_words = stop_words\n",
    "        self.vocabulary_ = None\n",
    "\n",
    "    def fit(self, raw_documents):\n",
    "        if self.lowercase:\n",
    "            raw_documents = [doc.lower() for doc in raw_documents]\n",
    "                    \n",
    "        combined_sentences = ' '.join(raw_documents)\n",
    "        all_tokens = combined_sentences.split(' ')\n",
    "        \n",
    "        # add stop words check\n",
    "        stop_words = self.stop_words\n",
    "        if stop_words == 'english':\n",
    "            stop_words = CountVectorizer.ENGLISH_STOP_WORDS\n",
    "        \n",
    "        # keep all terms if stop_words is None\n",
    "        if stop_words:\n",
    "            distinct_tokens = [token for token in set(all_tokens) if token not in stop_words]\n",
    "        else:\n",
    "            distinct_tokens = set(all_tokens)\n",
    "    \n",
    "        sorted_tokens = sorted(list(distinct_tokens))\n",
    "        self.vocabulary_ = sorted_tokens\n",
    "        \n",
    "        return self\n",
    "    \n",
    "cv = CountVectorizer(stop_words=['a', 'of'])\n",
    "cv.fit(text)\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we set the attributes of CountVectorizer a single time when instantiating CountVectorizer. We then can reuse all of the attributes on self. In addition to using self.lower_case, we are also using self.stop_words.\n",
    "\n",
    "We also want to include a static list of stop words that will be the same for all instances of CountVectorizer. In this case, we do not include these stop words (i.e. ENGLISH_STOP_WORDS) as part of \\__init\\__, since this is usually for instance specific attributes. Rather, we set it as part of the class, only indented in Countvectorizer, but not inside a method. \n",
    "\n",
    "To call ENGLISH_STOP_WORDS, we can use the class without ever creating an instance of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'of', 'in', 'the', 'to']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['again', 'and', 'document', 'first', 'is', 'it', 'one', 'second', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# use ENGLISH_STOP_WORDS by setting stop_words = 'english'\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(text)\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'is', 'it', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# change ENGLISH_STOP_WORDS\n",
    "CountVectorizer.ENGLISH_STOP_WORDS = ['again','and']\n",
    "\n",
    "# since ENGLISH_STOP_WORDS changes the entire class, previous instances are impacted\n",
    "cv.fit(text)\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['again', 'and', 'is', 'it', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# We can still add our own list of stop_words, ignoring ENGLISH_STOP_WORDS entirely\n",
    "cv = CountVectorizer(stop_words=['document','first'])\n",
    "cv.fit(text)\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add get_token_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'this': 2, 'is': 3, 'the': 4, 'first': 2, 'document': 3, 'second': 2, 'and': 1, 'third': 1, 'one': 1, 'it': 1, 'again': 1})\n"
     ]
    }
   ],
   "source": [
    "def get_token_stats(raw_documents, lowercase=True):\n",
    "    if lowercase:\n",
    "        raw_documents = [doc.lower() for doc in raw_documents]\n",
    "\n",
    "    token_stats = defaultdict(int)\n",
    "    for doc in raw_documents:\n",
    "        for term in doc.split(' '):\n",
    "            token_stats[term] += 1\n",
    "\n",
    "    sorted_tokens_with_count = sorted(token_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_tokens = [token[0] for token in sorted_tokens_with_count]\n",
    "\n",
    "    return token_stats, sorted_tokens\n",
    "\n",
    "token_stats, frequent_tokens = get_token_stats(text)\n",
    "print(token_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'is', 'the']\n"
     ]
    }
   ],
   "source": [
    "def fit(raw_documents, max_features=None, frequent_tokens=None):\n",
    "    \"\"\"\n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    :param max_features: int or None, default=None\n",
    "        If not None, build a vocabulary that only consider the top\n",
    "        max_features ordered by term frequency across the corpus\n",
    "    :param frequent_tokens: tokens ordered by frequency in the raw_documents\n",
    "        Tokens with the same frequency are in a random order, accordingly\n",
    "        tokens that display only once are not ordered alphabetically\n",
    "    \n",
    "    :return sorted_tokens: tokens sorted by frequency\n",
    "    \"\"\"\n",
    "\n",
    "    combined_sentences = ' '.join(raw_documents)\n",
    "    all_tokens = combined_sentences.split(' ')\n",
    "    distinct_tokens = set(all_tokens)\n",
    "\n",
    "    if max_features:\n",
    "        tokens_to_keep = frequent_tokens[0: max_features]\n",
    "        distinct_tokens = [token for token in distinct_tokens if token in tokens_to_keep]\n",
    "\n",
    "    sorted_tokens = sorted(list(distinct_tokens))\n",
    "\n",
    "    return sorted_tokens\n",
    "\n",
    "sorted_tokens = fit(text, max_features=5, frequent_tokens=frequent_tokens)\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['again', 'and', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "class CountVectorizer:\n",
    "    ENGLISH_STOP_WORDS = ['a', 'of', 'in', 'the', 'to']\n",
    "    \n",
    "    def __init__(self, lowercase=True, stop_words=None, max_features=None):\n",
    "        self.lowercase = lowercase\n",
    "        self.stop_words = stop_words\n",
    "        self.max_features = max_features\n",
    "        self.vocabulary_ = None\n",
    "        self.token_stats_ = None\n",
    "        self.frequent_tokens_ = None\n",
    "\n",
    "    def fit(self, raw_documents):\n",
    "        stop_words = self.stop_words\n",
    "        max_features = self.max_features\n",
    "        \n",
    "        if self.lowercase:\n",
    "            raw_documents = [doc.lower() for doc in raw_documents]\n",
    "                    \n",
    "        combined_sentences = ' '.join(raw_documents)\n",
    "        all_tokens = combined_sentences.split(' ')\n",
    "        \n",
    "        # add stop words check\n",
    "        if stop_words == 'english':\n",
    "            stop_words = CountVectorizer.ENGLISH_STOP_WORDS\n",
    "        \n",
    "        if stop_words:\n",
    "            distinct_tokens = [token for token in set(all_tokens) if token not in stop_words]\n",
    "        else:\n",
    "            distinct_tokens = set(all_tokens)\n",
    "\n",
    "        if self.frequent_tokens_ is None:\n",
    "            self._get_token_stats(raw_documents)\n",
    "\n",
    "        if max_features:\n",
    "            tokens_to_keep = self.frequent_tokens_[0: max_features]\n",
    "            distinct_tokens = [token for token in distinct_tokens if token in tokens_to_keep]\n",
    "    \n",
    "        sorted_tokens = sorted(list(distinct_tokens))\n",
    "        self.vocabulary_ = sorted_tokens\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def _get_token_stats(self, raw_documents):\n",
    "        token_stats = defaultdict(int)\n",
    "        for doc in raw_documents:\n",
    "            for term in doc.split(' '):\n",
    "                token_stats[term] += 1\n",
    "\n",
    "        frequent_tokens_with_count = sorted(token_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "        frequent_tokens = [token[0] for token in frequent_tokens_with_count]\n",
    "\n",
    "        self.token_stats_ = token_stats\n",
    "        self.frequent_tokens_ = frequent_tokens\n",
    "        \n",
    "        return self\n",
    "        \n",
    "cv = CountVectorizer(stop_words=['a', 'of'])\n",
    "cv.fit(text)\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2, 3), (0, 3, 4), (0, 1, 2), (0, 0, 3), (1, 2, 3), (1, 3, 4), (1, 0, 3), (2, 3, 4), (3, 3, 4), (3, 1, 2), (3, 0, 3)]\n"
     ]
    }
   ],
   "source": [
    "def transform(raw_documents, sorted_tokens, token_stats):\n",
    "    \"\"\"\n",
    "    :param raw_documents: iterable over raw text documents\n",
    "    :param sorted_tokens: tokens sorted by frequency\n",
    "    :param token_stats: tuple of (token:count)\n",
    "    \n",
    "    :return sparse_matrix: list of tuples with replicating a sparse matrix\n",
    "        indicates the index of non-zero tokens in a dense matrix: (row_num, column_num, count)\n",
    "    \"\"\"\n",
    "\n",
    "    sparse_matrix = []\n",
    "    tokens_col_index = {token:ind for ind, token in enumerate(sorted_tokens)}\n",
    "\n",
    "    for row_num, doc in enumerate(raw_documents):\n",
    "        for token in doc.split(' '):\n",
    "            if token in sorted_tokens:\n",
    "                sparse_matrix.append((row_num, tokens_col_index[token], token_stats[token]))\n",
    "    \n",
    "    return sparse_matrix\n",
    "\n",
    "sparse_matrix = transform(text, sorted_tokens=sorted_tokens, token_stats=token_stats)\n",
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4, 3), (0, 3, 2), (0, 2, 3), (1, 4, 3), (1, 7, 2), (1, 7, 2), (1, 2, 3), (2, 8, 1), (2, 6, 1), (3, 5, 1), (3, 3, 2), (3, 2, 3), (3, 0, 1)]\n"
     ]
    }
   ],
   "source": [
    "class CountVectorizer:\n",
    "    ENGLISH_STOP_WORDS = ['a', 'of', 'in', 'the', 'to']\n",
    "    \n",
    "    def __init__(self, lowercase=True, stop_words=None, max_features=None):\n",
    "        self.lowercase = lowercase\n",
    "        self.stop_words = stop_words\n",
    "        self.max_features = max_features\n",
    "        self.vocabulary_ = None\n",
    "        self.token_stats_ = None\n",
    "        self.frequent_tokens_ = None\n",
    "\n",
    "    def fit(self, raw_documents):\n",
    "        stop_words = self.stop_words\n",
    "        max_features = self.max_features\n",
    "        \n",
    "        if self.lowercase:\n",
    "            raw_documents = [doc.lower() for doc in raw_documents]\n",
    "                    \n",
    "        combined_sentences = ' '.join(raw_documents)\n",
    "        all_tokens = combined_sentences.split(' ')\n",
    "        \n",
    "        # add stop words check\n",
    "        if stop_words == 'english':\n",
    "            stop_words = CountVectorizer.ENGLISH_STOP_WORDS\n",
    "        \n",
    "        if stop_words:\n",
    "            distinct_tokens = [token for token in set(all_tokens) if token not in stop_words]\n",
    "        else:\n",
    "            distinct_tokens = set(all_tokens)\n",
    "\n",
    "\n",
    "        if self.frequent_tokens_ is None:\n",
    "            self._get_token_stats(raw_documents)\n",
    "\n",
    "        if max_features:\n",
    "            tokens_to_keep = self.frequent_tokens_[0: max_features]\n",
    "            distinct_tokens = [token for token in distinct_tokens if token in tokens_to_keep]\n",
    "    \n",
    "        sorted_tokens = sorted(list(distinct_tokens))\n",
    "        self.vocabulary_ = sorted_tokens\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def _get_token_stats(self, raw_documents):\n",
    "        token_stats = defaultdict(int)\n",
    "        for doc in raw_documents:\n",
    "            for term in doc.split(' '):\n",
    "                token_stats[term] += 1\n",
    "\n",
    "        frequent_tokens_with_count = sorted(token_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "        frequent_tokens = [token[0] for token in frequent_tokens_with_count]\n",
    "\n",
    "        self.token_stats_ = token_stats\n",
    "        self.frequent_tokens_ = frequent_tokens\n",
    "        \n",
    "        return self\n",
    " \n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"\n",
    "        :param raw_documents: iterable over raw text documents\n",
    "        :param sorted_tokens: tokens sorted by frequency\n",
    "        :param token_stats: tuple of (token:count)\n",
    "\n",
    "        :return sparse_matrix: list of tuples with replicating a sparse matrix\n",
    "            indicates the index of non-zero tokens in a dense matrix: (row_num, column_num, count)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.vocabulary_ is None:\n",
    "            raise('Must run fit before transform')\n",
    "\n",
    "        sorted_tokens = self.vocabulary_\n",
    "        token_stats = self.token_stats_\n",
    "\n",
    "        sparse_matrix = []\n",
    "        tokens_col_index = {token:ind for ind, token in enumerate(sorted_tokens)}\n",
    "        for row_num, doc in enumerate(raw_documents):\n",
    "            for token in doc.split(' '):\n",
    "                if token in sorted_tokens:\n",
    "                    sparse_matrix.append((row_num, tokens_col_index[token], token_stats[token]))\n",
    "\n",
    "        return sparse_matrix\n",
    "\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(text)\n",
    "sparse_matrix = cv.transform(text)\n",
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['again', 'and', 'document', 'first', 'is', 'it', 'one', 'second', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "class CountVectorizer:\n",
    "    ENGLISH_STOP_WORDS = ['a', 'of', 'in', 'the', 'to']\n",
    "    \n",
    "    def __init__(self, lowercase=True, stop_words=None, max_features=None):\n",
    "        self.lowercase = lowercase\n",
    "        self.stop_words = stop_words\n",
    "        self.max_features = max_features\n",
    "        self.vocabulary_ = None\n",
    "        self.token_stats_ = None\n",
    "        self.frequent_tokens_ = None\n",
    "\n",
    "    def fit(self, raw_documents):\n",
    "        stop_words = self.stop_words\n",
    "        max_features = self.max_features\n",
    "        \n",
    "        if self.lowercase:\n",
    "            raw_documents = [doc.lower() for doc in raw_documents]\n",
    "                    \n",
    "        combined_sentences = ' '.join(raw_documents)\n",
    "        all_tokens = combined_sentences.split(' ')\n",
    "        \n",
    "        # add stop words check\n",
    "        if stop_words == 'english':\n",
    "            stop_words = CountVectorizer.ENGLISH_STOP_WORDS\n",
    "        \n",
    "        if stop_words:\n",
    "            distinct_tokens = [token for token in set(all_tokens) if token not in stop_words]\n",
    "        else:\n",
    "            distinct_tokens = set(all_tokens)\n",
    "\n",
    "\n",
    "        if self.frequent_tokens_ is None:\n",
    "            self._get_token_stats(raw_documents)\n",
    "\n",
    "        if max_features:\n",
    "            tokens_to_keep = self.frequent_tokens_[0: max_features]\n",
    "            distinct_tokens = [token for token in distinct_tokens if token in tokens_to_keep]\n",
    "    \n",
    "        sorted_tokens = sorted(list(distinct_tokens))\n",
    "        self.vocabulary_ = sorted_tokens\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def _get_token_stats(self, raw_documents):\n",
    "        token_stats = defaultdict(int)\n",
    "        for doc in raw_documents:\n",
    "            for term in doc.split(' '):\n",
    "                token_stats[term] += 1\n",
    "\n",
    "        frequent_tokens_with_count = sorted(token_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "        frequent_tokens = [token[0] for token in frequent_tokens_with_count]\n",
    "\n",
    "        self.token_stats_ = token_stats\n",
    "        self.frequent_tokens_ = frequent_tokens\n",
    "        \n",
    "        return self\n",
    " \n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"\n",
    "        :param raw_documents: iterable over raw text documents\n",
    "        :param sorted_tokens: tokens sorted by frequency\n",
    "        :param token_stats: tuple of (token:count)\n",
    "\n",
    "        :return sparse_matrix: list of tuples with replicating a sparse matrix\n",
    "            indicates the index of non-zero tokens in a dense matrix: (row_num, column_num, count)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.vocabulary_ is None:\n",
    "            raise('Must run fit before transform')\n",
    "\n",
    "        sorted_tokens = self.vocabulary_\n",
    "        token_stats = self.token_stats_\n",
    "\n",
    "        sparse_matrix = []\n",
    "        tokens_col_index = {token:ind for ind, token in enumerate(sorted_tokens)}\n",
    "        for row_num, doc in enumerate(raw_documents):\n",
    "            for token in doc.split(' '):\n",
    "                if token in sorted_tokens:\n",
    "                    sparse_matrix.append((row_num, tokens_col_index[token], token_stats[token]))\n",
    "\n",
    "        return sparse_matrix\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\" Get an alphabetical list of the vocabulary learned in the fit method \"\"\"\n",
    "\n",
    "        return [token for token in sorted(self.vocabulary_)]\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(text)\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the below code will place greater focus on learning Python classes than exactly replicating the CountVectorizer codebase. Thus, the code will have substantial differences in places. The primary differences are do to the following reasons:\n",
    "- Error handling - To reduce user errors, scikit-learn has includes substantial error handling code\n",
    "- Optimization - scikit-learn optimizes the code to reduce memory usage and reduce necessary calculations\n",
    "- Compatibility - scikit-learn makes sure that the codebase is compatible with multiple versions of python\n",
    "- Inheritance - other classes (e.g. TFIDFVectorizer) reuse much of the code of CountVectorizer, so scikit-learn uses inheritance (we will not cover that here) to reduce code and encourage code resue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA MATERIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\__repr\\__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the fit method, we have used 'return self'. This returns a representation of the class instance, which we can change with the \\__repr\\__ method. \n",
    "\n",
    "Using \\__repr\\__, we can print out our own string to explain the class instance; this will help us replicate how the scikit-learn CountVectorizer printed out all the set parameters for the CountVectorizer instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CountVectorizer at 0x20e6e9b2908>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CountVectorizer at 0x20e6e9b2c88>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let review a simplified example\n",
    "\n",
    "class CountVectorizer:\n",
    "\n",
    "    def __init__(self, lowercase=True):\n",
    "        self.lowercase = lowercase\n",
    "        \n",
    "cv = CountVectorizer()\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the class instance prints the name of the class and memory related information. We will update this to say something more meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(lowercase=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's add a __repr__ method to change the output when we print the class instance\n",
    "\n",
    "class CountVectorizer:\n",
    "    def __init__(self, lowercase=True):\n",
    "        self.lowercase = lowercase\n",
    "\n",
    "    # add a __repr__\n",
    "    def __repr__(self):\n",
    "        return \"CountVectorizer(lowercase={})\".format(self.lowercase)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's add a fit method without 'return self'\n",
    "\n",
    "class CountVectorizer:\n",
    "    def __init__(self, lowercase=True):\n",
    "        self.lowercase = lowercase\n",
    "    \n",
    "    # add a fit method\n",
    "    def fit(self, raw_documents):\n",
    "        # simplified fit\n",
    "        self.vocabulary_ = raw_documents\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"CountVectorizer(lowercase={})\".format(self.lowercase)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the cv.fit(text) no output displays. This is because the method does not include a return value or print statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(lowercase=True)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's add a fit method without 'return self'\n",
    "\n",
    "class CountVectorizer:\n",
    "    def __init__(self, lowercase=True):\n",
    "        self.lowercase = lowercase\n",
    "    \n",
    "    # add a fit method\n",
    "    def fit(self, raw_documents):\n",
    "        self.vocabulary_ = raw_documents\n",
    "    \n",
    "        # add return self\n",
    "        return self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"CountVectorizer(lowercase={})\".format(self.lowercase)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we add 'return self' to the fit method, we print out the \\__repr\\__ value by default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(lowercase=True, max_features=None, stop_words=english)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put everything together with a complete __repr__ method (found at the end of the class)\n",
    "\n",
    "class CountVectorizer:\n",
    "    ENGLISH_STOP_WORDS = ['a', 'of', 'in', 'the', 'to']\n",
    "    \n",
    "    def __init__(self, lowercase=True, stop_words=None, max_features=None):\n",
    "        self.lowercase = lowercase\n",
    "        self.stop_words = stop_words\n",
    "        self.max_features = max_features\n",
    "        self.vocabulary_ = None\n",
    "        self.token_stats_ = None\n",
    "        self.frequent_tokens_ = None\n",
    "\n",
    "    def fit(self, raw_documents):\n",
    "        stop_words = self.stop_words\n",
    "        max_features = self.max_features\n",
    "        \n",
    "        if self.lowercase:\n",
    "            raw_documents = [doc.lower() for doc in raw_documents]\n",
    "                    \n",
    "        combined_sentences = ' '.join(raw_documents)\n",
    "        all_tokens = combined_sentences.split(' ')\n",
    "        \n",
    "        if stop_words == 'english':\n",
    "            stop_words = CountVectorizer.ENGLISH_STOP_WORDS\n",
    "        \n",
    "        if stop_words:\n",
    "            distinct_tokens = [token for token in set(all_tokens) if token not in stop_words]\n",
    "        else:\n",
    "            distinct_tokens = set(all_tokens)\n",
    "\n",
    "\n",
    "        if self.frequent_tokens_ is None:\n",
    "            self._get_token_stats(raw_documents)\n",
    "\n",
    "        if max_features:\n",
    "            tokens_to_keep = self.frequent_tokens_[0: max_features]\n",
    "            distinct_tokens = [token for token in distinct_tokens if token in tokens_to_keep]\n",
    "    \n",
    "        sorted_tokens = sorted(list(distinct_tokens))\n",
    "        self.vocabulary_ = sorted_tokens\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def _get_token_stats(self, raw_documents):\n",
    "        token_stats = defaultdict(int)\n",
    "        for doc in raw_documents:\n",
    "            for term in doc.split(' '):\n",
    "                token_stats[term] += 1\n",
    "\n",
    "        frequent_tokens_with_count = sorted(token_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "        frequent_tokens = [token[0] for token in frequent_tokens_with_count]\n",
    "\n",
    "        self.token_stats_ = token_stats\n",
    "        self.frequent_tokens_ = frequent_tokens\n",
    "        \n",
    "        return self\n",
    " \n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"\n",
    "        :param raw_documents: iterable over raw text documents\n",
    "        :param sorted_tokens: tokens sorted by frequency\n",
    "        :param token_stats: tuple of (token:count)\n",
    "\n",
    "        :return sparse_matrix: list of tuples with replicating a sparse matrix\n",
    "            indicates the index of non-zero tokens in a dense matrix: (row_num, column_num, count)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.vocabulary_ is None:\n",
    "            raise('Must run fit before transform')\n",
    "\n",
    "        sorted_tokens = self.vocabulary_\n",
    "        token_stats = self.token_stats_\n",
    "\n",
    "        sparse_matrix = []\n",
    "        tokens_col_index = {token:ind for ind, token in enumerate(sorted_tokens)}\n",
    "        for row_num, doc in enumerate(raw_documents):\n",
    "            for token in doc.split(' '):\n",
    "                if token in sorted_tokens:\n",
    "                    sparse_matrix.append((row_num, tokens_col_index[token], token_stats[token]))\n",
    "\n",
    "        return sparse_matrix\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"CountVectorizer(lowercase={}, max_features={}, stop_words={})\".format(\n",
    "            self.lowercase, self.max_features, self.stop_words)\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_features=1.5, neither a positive integer nor None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-9e58d3326124>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_features={}, neither a positive integer nor None\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-9e58d3326124>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lowercase, stop_words, max_features)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_features={}, neither a positive integer nor None\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: max_features=1.5, neither a positive integer nor None"
     ]
    }
   ],
   "source": [
    "class CountVectorizer:\n",
    "    \n",
    "    def __init__(self, lowercase=True, stop_words=None, max_features=None):\n",
    "        self.lowercase = lowercase\n",
    "        self.stop_words = stop_words\n",
    "        self.max_features = max_features\n",
    "        \n",
    "        if max_features is not None:\n",
    "            if (not isinstance(max_features, int)) or (max_features <= 0):\n",
    "                raise ValueError(\"max_features={}, neither a positive integer nor None\".format(max_features))\n",
    "        \n",
    "cv = CountVectorizer(max_features=1.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
