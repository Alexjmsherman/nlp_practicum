{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning for NLP 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python [conda env:guild] *",
      "language": "python",
      "name": "conda-env-guild-py"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexjmsherman/nlp_practicum_cohort3_student/blob/master/Deep_Learning_for_NLP_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNfyZNVE8c8g",
        "colab_type": "text"
      },
      "source": [
        "## Deep Learning for NLP\n",
        "\n",
        "##### Author: Alex Sherman | alsherman@deloitte.com\n",
        "\n",
        "\n",
        "Agenda:\n",
        "- Custom embeddings\n",
        "- CNN\n",
        "- LSTM\n",
        "- Contextual Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsPK_oW08c8i",
        "colab_type": "code",
        "outputId": "8e7a4f5c-cc9a-469d-e7a4-20ce7d77dda6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from numpy import zeros\n",
        "import pandas as pd\n",
        "import requests\n",
        "from zipfile import ZipFile \n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, \\\n",
        "    Embedding, Input, GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, GlobalMaxPool1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYe-0extLPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file names for lesson\n",
        "\n",
        "NIH_EXPORTER_CSV = r'exporter_train_data.csv'\n",
        "URL = r'https://exporter.nih.gov/ExPORTER_Catalog.aspx'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQClJWAdOeRG",
        "colab_type": "code",
        "outputId": "6682b4ea-c2da-4795-8302-f34d24cc282d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "# optional  - mount google drive to save data (to avoid repeating large file downloads)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foYdukBi8c8n",
        "colab_type": "text"
      },
      "source": [
        "# Problem Definition\n",
        "\n",
        "Predict the National Institutes of Health (NIH) Institute of Center (IC) from the Project Title of previously funded projects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0H32ZSAhzXi",
        "colab_type": "text"
      },
      "source": [
        "### Download and Store Data \n",
        "\n",
        "##### WARNING - LARGE DOWNLOAD (1 HR+)\n",
        "- It is recommended to skip this section as data is provided separately below\n",
        "\n",
        "- If you want to run the code in this section you must uncomment !wget which is commented to avoid accidental large file downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5waC11a6tub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the csv names of all project csvs at https://exporter.nih.gov\n",
        "\n",
        "r = requests.get(URL)\n",
        "b = BeautifulSoup(r.text)\n",
        "table = b.find('table', attrs={'class':'header_band_bg'})\n",
        "\n",
        "for row in table.find('table').find_all('tr', attrs={'class':'row_bg'}):\n",
        "    row_cells = row.find_all('td')\n",
        "    fname = row_cells[0].text.strip()\n",
        "    csv_url = 'https://exporter.nih.gov/' + row_cells[4].find('a')['href']\n",
        "    \n",
        "    # download file\n",
        "    #!wget {csv_url}     # UNCOMMENT TO DOWNLOAD FILES (COMMENTED OUT AS A SAFEGUARD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlQKSRpcBQlz",
        "colab_type": "code",
        "outputId": "e1c6cd49-2aa5-4c32-8cd8-0a4402c66fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# filter downloaded csvs to relevant columns and save data\n",
        "\n",
        "# only include .zip files\n",
        "nih_project_csvs = [f for f in os.listdir() if f.endswith('.zip')]\n",
        "\n",
        "for ind, f in enumerate(nih_project_csvs):\n",
        "\n",
        "    # view the data\n",
        "    df = pd.read_csv(f, encoding='latin-1')\n",
        "\n",
        "    # filter to relevant columns\n",
        "    df = df[['ADMINISTERING_IC', 'FY',  'IC_NAME', 'PROJECT_TITLE']]\n",
        "    df['SOURCE'] = f\n",
        "    \n",
        "    # only save column headers for the first file\n",
        "    if ind == 0:\n",
        "        df.to_csv(NIH_EXPORTER_CSV, index=False, mode='w+')\n",
        "    else:\n",
        "        df.to_csv(NIH_EXPORTER_CSV, index=False, header=None, mode='a')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 243 µs, sys: 39 µs, total: 282 µs\n",
            "Wall time: 200 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hscC95S9PxYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5d5fa48d-29f6-4c9f-ccbf-bbc249efbfdc"
      },
      "source": [
        "# copy data to google drive\n",
        "# NOTE - you will need to create these folders (e.g. nih_data) in google drive for this to work\n",
        "\n",
        "!cp exporter.csv gdrive/My\\ Drive/Colab\\ Notebooks/nih_data/exporter_train_data.csv"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'exporter.csv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8MKhgNuiEnl",
        "colab_type": "text"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X_RvpHEvKR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load trainng data \n",
        "\n",
        "!cp gdrive/My\\ Drive/Colab\\ Notebooks/nih_data/exporter_train_data.csv .\n",
        "\n",
        "df = pd.read_csv(NIH_EXPORTER_CSV, encoding='latin-1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBgP7o3B4RNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load testing data\n",
        "\n",
        "!cp gdrive/My\\ Drive/Colab\\ Notebooks/nih_data/test_data_RePORTER_PRJ_C_FY2017.csv .\n",
        "\n",
        "test_df = pd.read_csv(\n",
        "    'test_data_RePORTER_PRJ_C_FY2017.csv', \n",
        "    encoding='latin-1',\n",
        "    header=0,\n",
        "    names=['PROJECT_TITLE','IC_NUM']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WgmTsCJouBo",
        "colab_type": "text"
      },
      "source": [
        "### Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1__WK_r4mCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove test data from training data\n",
        "test_file = df['SOURCE'] == 'RePORTER_PRJ_C_FY2017.zip'\n",
        "test_titles = df['PROJECT_TITLE'].isin(test_df['PROJECT_TITLE'])\n",
        "df = df[~(test_titles & test_file)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpAUelXSKbYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# limit to the most freqent institutes/centers (IC)\n",
        "# use same mapping order as testing data (from previous lesson)\n",
        "\n",
        "top_ic_names = {\n",
        " 'NATIONAL INSTITUTE OF MENTAL HEALTH': 0,\n",
        " 'NATIONAL CANCER INSTITUTE': 1,\n",
        " 'NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES': 2,\n",
        " 'NATIONAL INSTITUTE OF GENERAL MEDICAL SCIENCES': 3,\n",
        " 'NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE': 4,\n",
        " 'NATIONAL INSTITUTE ON AGING': 5,\n",
        " 'NATIONAL HEART, LUNG, AND BLOOD INSTITUTE': 6,\n",
        " 'NATIONAL INSTITUTE OF DIABETES AND DIGESTIVE AND KIDNEY DISEASES': 7,\n",
        " 'EUNICE KENNEDY SHRIVER NATIONAL INSTITUTE OF CHILD HEALTH & HUMAN DEVELOPMENT': 8,\n",
        " 'NATIONAL EYE INSTITUTE': 9,\n",
        " 'NATIONAL INSTITUTE ON DRUG ABUSE': 10,\n",
        " 'NATIONAL INSTITUTE OF ALLERGY AND INFECTIOUS DISEASES': 11,\n",
        " 'NATIONAL INSTITUTE OF ARTHRITIS AND MUSCULOSKELETAL AND SKIN DISEASES': 12\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zku1iSsYEGaq",
        "colab_type": "code",
        "outputId": "a5188cc1-3b1c-4028-b078-695d413a1a0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "# filter data to most frequent ICs\n",
        "df = df[df['IC_NAME'].isin(top_ic_names)]\n",
        "df = df[df['PROJECT_TITLE'].notnull()]\n",
        "\n",
        "# set the labels as a new column\n",
        "df['IC_NUM'] = df['IC_NAME'].map(top_ic_names)\n",
        "\n",
        "# create a map of IC nums to names for later reference\n",
        "ic_name_map = {num:name for num, name in df[['IC_NUM','IC_NAME']].drop_duplicates().values}\n",
        "\n",
        "# view data\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1686687, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ADMINISTERING_IC</th>\n",
              "      <th>FY</th>\n",
              "      <th>IC_NAME</th>\n",
              "      <th>PROJECT_TITLE</th>\n",
              "      <th>SOURCE</th>\n",
              "      <th>IC_NUM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DA</td>\n",
              "      <td>2018</td>\n",
              "      <td>NATIONAL INSTITUTE ON DRUG ABUSE</td>\n",
              "      <td>HIV and Other Infectious Consequences of Subst...</td>\n",
              "      <td>RePORTER_PRJ_C_FY2019_009.zip</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MH</td>\n",
              "      <td>2019</td>\n",
              "      <td>NATIONAL INSTITUTE OF MENTAL HEALTH</td>\n",
              "      <td>Predictive Coding as a Framework for Understan...</td>\n",
              "      <td>RePORTER_PRJ_C_FY2019_009.zip</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HL</td>\n",
              "      <td>2018</td>\n",
              "      <td>NATIONAL HEART, LUNG, AND BLOOD INSTITUTE</td>\n",
              "      <td>The role of the gut microbiome-host metabolome...</td>\n",
              "      <td>RePORTER_PRJ_C_FY2019_009.zip</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AI</td>\n",
              "      <td>2019</td>\n",
              "      <td>NATIONAL INSTITUTE OF ALLERGY AND INFECTIOUS D...</td>\n",
              "      <td>Liver resident memory for malaria</td>\n",
              "      <td>RePORTER_PRJ_C_FY2019_009.zip</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AI</td>\n",
              "      <td>2019</td>\n",
              "      <td>NATIONAL INSTITUTE OF ALLERGY AND INFECTIOUS D...</td>\n",
              "      <td>Novel Biomolecular and Biophysical Mechanisms ...</td>\n",
              "      <td>RePORTER_PRJ_C_FY2019_009.zip</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ADMINISTERING_IC    FY  ...                         SOURCE IC_NUM\n",
              "0               DA  2018  ...  RePORTER_PRJ_C_FY2019_009.zip     10\n",
              "1               MH  2019  ...  RePORTER_PRJ_C_FY2019_009.zip      0\n",
              "2               HL  2018  ...  RePORTER_PRJ_C_FY2019_009.zip      6\n",
              "3               AI  2019  ...  RePORTER_PRJ_C_FY2019_009.zip     11\n",
              "4               AI  2019  ...  RePORTER_PRJ_C_FY2019_009.zip     11\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3sovYv28c9A",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess data and create Train/Test splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEnO8gl_8c9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\" use gensim simple_preprocess to tokenize text. Join results back \n",
        "    into a clean text string\n",
        "    \n",
        "    :param text: string, text to preprocess\n",
        "    :return clean_text: string, cleaned text\n",
        "    \"\"\"\n",
        "    \n",
        "    clean_tokens = simple_preprocess(text)\n",
        "    clean_text = ' '.join(clean_tokens)\n",
        "    \n",
        "    return clean_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSNVrE9E8c9L",
        "colab_type": "code",
        "outputId": "b3cc8cf9-1467-4efd-a481-b0cadcb66806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# separate the features and response\n",
        "X_train = df['PROJECT_TITLE'].apply(lambda x: preprocess_text(x))\n",
        "y_train = df['IC_NUM']\n",
        "X_test = test_df['PROJECT_TITLE'].apply(lambda x: preprocess_text(x))\n",
        "y_test = test_df['IC_NUM']\n",
        "\n",
        "# get a count of the number of possible categories to predict\n",
        "num_classes = len(set(y_train))\n",
        "\n",
        "# convert the training and testing dataset\n",
        "y_train_array = to_categorical(y_train, num_classes)\n",
        "y_test_array = to_categorical(y_test, num_classes)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 17.3 s, sys: 108 ms, total: 17.4 s\n",
            "Wall time: 17.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf4xx9oV2So9",
        "colab_type": "text"
      },
      "source": [
        "## Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpwJifFueObM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import  RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXuzyNHMW8Wg",
        "colab_type": "code",
        "outputId": "af2ce90e-57f1-4d74-fc4b-61d26c38d4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "%%time \n",
        "\n",
        "# NLP Pipeline\n",
        "pipe = Pipeline([\n",
        "      ('tfidf', TfidfVectorizer())\n",
        "    , ('clf', LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "# Hyperparameters to test\n",
        "param_dist = {\n",
        "       #  tfidf hyperparams\n",
        "         'tfidf__max_features': [15000, 20000, 25000, 30000]\n",
        "       , 'tfidf__ngram_range': [(1,1),(1,2)]\n",
        "       \n",
        "       #   logistic regression hyperparams\n",
        "       ,  'clf__penalty':['l1','l2']\n",
        "       ,  'clf__C':np.linspace(.01, 5)\n",
        "}\n",
        "\n",
        "# run experiments to determine best pipeline\n",
        "grid = RandomizedSearchCV(\n",
        "      pipe\n",
        "    , param_distributions=param_dist\n",
        "    , n_iter=5\n",
        "    , cv=3\n",
        "    , refit='neg_log_loss'\n",
        "    , scoring=['accuracy','neg_log_loss','precision_macro','recall_macro','f1_macro']\n",
        "    , return_train_score=True\n",
        "    , error_score=0\n",
        "    , n_jobs=-1\n",
        "    , verbose=2\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)  # save testing data for final evaluation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 207.9min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 37min 27s, sys: 20min 47s, total: 58min 15s\n",
            "Wall time: 3h 59min 54s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIXswwHsZLRn",
        "colab_type": "code",
        "outputId": "a98452c2-9cd5-4288-f93b-0e0f40d3ad3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "# evaluate results\n",
        "pd.DataFrame(grid.cv_results_)[[\n",
        "  'mean_test_accuracy',\n",
        "  'mean_test_f1_macro',\n",
        "  'mean_test_neg_log_loss',\n",
        "  'mean_test_precision_macro',\n",
        "  'mean_test_recall_macro',\n",
        "  'param_tfidf__max_features',\n",
        "  'param_clf__C',\n",
        "  'mean_fit_time',\n",
        "  'mean_score_time'\n",
        "]].sort_values('mean_test_accuracy', ascending=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_test_accuracy</th>\n",
              "      <th>mean_test_f1_macro</th>\n",
              "      <th>mean_test_neg_log_loss</th>\n",
              "      <th>mean_test_precision_macro</th>\n",
              "      <th>mean_test_recall_macro</th>\n",
              "      <th>param_tfidf__max_features</th>\n",
              "      <th>param_clf__C</th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.773230</td>\n",
              "      <td>0.757626</td>\n",
              "      <td>-0.742971</td>\n",
              "      <td>0.787218</td>\n",
              "      <td>0.737926</td>\n",
              "      <td>20000</td>\n",
              "      <td>3.57429</td>\n",
              "      <td>2091.406558</td>\n",
              "      <td>53.016214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.772749</td>\n",
              "      <td>0.756865</td>\n",
              "      <td>-0.745863</td>\n",
              "      <td>0.786829</td>\n",
              "      <td>0.737022</td>\n",
              "      <td>20000</td>\n",
              "      <td>3.26878</td>\n",
              "      <td>2093.881857</td>\n",
              "      <td>56.851987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.768783</td>\n",
              "      <td>0.750875</td>\n",
              "      <td>-0.768219</td>\n",
              "      <td>0.783904</td>\n",
              "      <td>0.729930</td>\n",
              "      <td>20000</td>\n",
              "      <td>1.84306</td>\n",
              "      <td>1720.262235</td>\n",
              "      <td>47.069106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.757975</td>\n",
              "      <td>0.737643</td>\n",
              "      <td>-0.807757</td>\n",
              "      <td>0.774065</td>\n",
              "      <td>0.715571</td>\n",
              "      <td>15000</td>\n",
              "      <td>1.1302</td>\n",
              "      <td>1354.656613</td>\n",
              "      <td>56.133210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.625693</td>\n",
              "      <td>0.553095</td>\n",
              "      <td>-1.479332</td>\n",
              "      <td>0.661726</td>\n",
              "      <td>0.515852</td>\n",
              "      <td>15000</td>\n",
              "      <td>0.01</td>\n",
              "      <td>213.455879</td>\n",
              "      <td>55.952687</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_test_accuracy  mean_test_f1_macro  ...  mean_fit_time  mean_score_time\n",
              "0            0.773230            0.757626  ...    2091.406558        53.016214\n",
              "3            0.772749            0.756865  ...    2093.881857        56.851987\n",
              "4            0.768783            0.750875  ...    1720.262235        47.069106\n",
              "1            0.757975            0.737643  ...    1354.656613        56.133210\n",
              "2            0.625693            0.553095  ...     213.455879        55.952687\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBwuhx_n8c_h",
        "colab_type": "text"
      },
      "source": [
        "# Full Embedding and Model Pipeline Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMSDpD7I8c_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbeddingModel:\n",
        "    \n",
        "    def __init__(self, X_train, X_test, y_train, y_test, \n",
        "                 max_num_words=20000, max_seq_length=50, \n",
        "                 embedding_size=50, embedding_dir=None):\n",
        "                \n",
        "        # set tokenizer params\n",
        "        self.max_num_words = max_num_words\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.vocab_size = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "        # format data\n",
        "        self.num_classes = len(set(y_train))        \n",
        "        self.y_train_array = to_categorical(y_train, self.num_classes)\n",
        "        self.y_test_array = to_categorical(y_test, self.num_classes)\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.X_train_sequence = self.encode_text(X_train, train=True)\n",
        "        self.X_test_sequence = self.encode_text(X_test, train=False)\n",
        "                \n",
        "        # set embedding params\n",
        "        self.embedding_dir = embedding_dir\n",
        "        self.embedding_size = embedding_size\n",
        "        self.embeddings_index = None\n",
        "        self.embedding_matrix = None\n",
        "        \n",
        "        # set model params\n",
        "        self.model = None\n",
        "    \n",
        "    def setup_model_pipeline(self):\n",
        "        self.create_embeddings_index()\n",
        "        self.create_embedding_matrix()\n",
        "        print('model pipeline set-up complete')\n",
        "\n",
        "    def encode_text(self, text, train=False):\n",
        "        if train:\n",
        "            self.tokenizer = Tokenizer(num_words=self.max_num_words)\n",
        "            self.tokenizer.fit_on_texts(text)\n",
        "\n",
        "        encoded_docs = self.tokenizer.texts_to_sequences(text)\n",
        "        padded_docs = pad_sequences(\n",
        "            encoded_docs,\n",
        "            maxlen=self.max_seq_length,\n",
        "            padding='post'\n",
        "        )\n",
        "\n",
        "        print(f'completed tokenizing and padding text - train: {train}')\n",
        "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
        "        return padded_docs\n",
        "\n",
        "    def create_embeddings_index(self):\n",
        "        embeddings_index = {}\n",
        "\n",
        "        with open(self.embedding_dir, 'rb') as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0].decode('utf-8')\n",
        "                embedding = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_index[word] = embedding\n",
        "\n",
        "        print('completed creating embedding index')\n",
        "        self.embeddings_index = embeddings_index\n",
        "\n",
        "    def create_embedding_matrix(self):\n",
        "        embedding_matrix = zeros((self.vocab_size, self.embedding_size))\n",
        "\n",
        "        for word, i in self.tokenizer.word_index.items():    \n",
        "            embedding_vector = self.embeddings_index.get(word)\n",
        "\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        print('completed creating embedding matrix')\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "\n",
        "    def get_embedding_layer(self):\n",
        "        embedding = Embedding(\n",
        "            input_dim=self.vocab_size, \n",
        "            output_dim=self.embedding_size,                                    \n",
        "            input_length=self.max_seq_length,\n",
        "            embeddings_initializer=Constant(self.embedding_matrix),\n",
        "            trainable=False                                   \n",
        "        )\n",
        "\n",
        "        return embedding\n",
        "\n",
        "    def fit(self, model=None, epochs=10):\n",
        "        if model:\n",
        "            print('using custom model')\n",
        "        else:\n",
        "            # default model if a custom model is not provided\n",
        "            model = Sequential()\n",
        "            model.add(self.get_embedding_layer())\n",
        "            model.add(Flatten())\n",
        "            model.add(Dense(self.num_classes, activation='softmax'))\n",
        "            model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        \n",
        "        # train model\n",
        "        model.fit(\n",
        "            self.X_train_sequence, \n",
        "            self.y_train_array,\n",
        "            epochs=epochs,\n",
        "            shuffle=True,\n",
        "            validation_data=(self.X_test_sequence, self.y_test_array)\n",
        "        )\n",
        "\n",
        "        print('completed training model')\n",
        "        self.model = model\n",
        "        \n",
        "    def predict(self, X):\n",
        "        encoded_text = self.encode_text(X, train=False)\n",
        "        y_pred = self.model.predict_classes(encoded_text)\n",
        "        \n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_meJQUt1Vid",
        "colab_type": "text"
      },
      "source": [
        "## Download Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IsWuPCPysQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLOVE_ZIP = 'glove.840B.300d.zip'\n",
        "GLOVE_DIR = 'glove.840B.300d.txt'\n",
        "EMBEDDING_SIZE = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUE4pn_G1nvV",
        "colab_type": "code",
        "outputId": "b39ed029-3f45-4418-fec7-80f599ae9611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "# uncomment below for 2GB GLoVe Embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-16 14:34:48--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2019-07-16 14:34:48--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2019-07-16 14:34:49--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  19.0MB/s    in 1m 51s  \n",
            "\n",
            "2019-07-16 14:36:40 (18.7 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpagepTo2Coo",
        "colab_type": "code",
        "outputId": "85321eae-25dc-481e-ea6c-8e5297bd95f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "from zipfile import ZipFile \n",
        "    \n",
        "# extract all embedding files from the zip \n",
        "with ZipFile(GLOVE_ZIP, 'r') as z:  \n",
        "    z.extractall()     "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 39.8 s, sys: 6.62 s, total: 46.4 s\n",
            "Wall time: 49.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ir8wISq7PqG",
        "colab_type": "code",
        "outputId": "6c19c730-fc6e-4eef-8e42-55377e354281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "!rm -rf glove.840B.300d.zip  # delete zip after extracting embeddings\n",
        "!ls"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "exporter_train_data.csv  sample_data\n",
            "gdrive\t\t\t test_data_RePORTER_PRJ_C_FY2017.csv\n",
            "glove.840B.300d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bKCZgkK4JSz",
        "colab_type": "text"
      },
      "source": [
        "## Test Embedding Model with Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LCMz3AB8c_m",
        "colab_type": "code",
        "outputId": "d9b9b60a-67bb-4ff2-acf4-8476dd1f4f93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# instantiate nlp model pipeline\n",
        "embedding_model = EmbeddingModel(\n",
        "    X_train=X_train, \n",
        "    X_test=X_test, \n",
        "    y_train=y_train, \n",
        "    y_test=y_test,\n",
        "    max_num_words=10000,\n",
        "    max_seq_length=60,\n",
        "    embedding_dir=GLOVE_DIR,\n",
        "    embedding_size=EMBEDDING_SIZE\n",
        ")\n",
        "\n",
        "# set-up and train model\n",
        "embedding_model.setup_model_pipeline()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed tokenizing and padding text - train: True\n",
            "completed tokenizing and padding text - train: False\n",
            "completed creating embedding index\n",
            "completed creating embedding matrix\n",
            "model pipeline set-up complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8i4wqQo8pDZ",
        "colab_type": "code",
        "outputId": "619e1884-e983-4c3b-98db-c71a905116fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        }
      },
      "source": [
        "embedding_model.fit(epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0701 18:48:42.567846 139943884109696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0701 18:48:42.627224 139943884109696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0701 18:48:43.094040 139943884109696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0701 18:48:43.110794 139943884109696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0701 18:48:43.130071 139943884109696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0701 18:48:43.260971 139943884109696 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0701 18:48:43.311816 139943884109696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1686687 samples, validate on 15341 samples\n",
            "Epoch 1/10\n",
            "1686687/1686687 [==============================] - 138s 82us/step - loss: 1.1912 - acc: 0.6427 - val_loss: 1.3057 - val_acc: 0.6244\n",
            "Epoch 2/10\n",
            "1686687/1686687 [==============================] - 133s 79us/step - loss: 1.1648 - acc: 0.6525 - val_loss: 1.2919 - val_acc: 0.6324\n",
            "Epoch 3/10\n",
            "1686687/1686687 [==============================] - 133s 79us/step - loss: 1.1647 - acc: 0.6531 - val_loss: 1.2979 - val_acc: 0.6342\n",
            "Epoch 4/10\n",
            "1686687/1686687 [==============================] - 133s 79us/step - loss: 1.1652 - acc: 0.6537 - val_loss: 1.3262 - val_acc: 0.6322\n",
            "Epoch 5/10\n",
            "1686687/1686687 [==============================] - 133s 79us/step - loss: 1.1658 - acc: 0.6538 - val_loss: 1.3063 - val_acc: 0.6326\n",
            "Epoch 6/10\n",
            "1686687/1686687 [==============================] - 133s 79us/step - loss: 1.1667 - acc: 0.6543 - val_loss: 1.3373 - val_acc: 0.6314\n",
            "Epoch 7/10\n",
            "1686687/1686687 [==============================] - 133s 79us/step - loss: 1.1669 - acc: 0.6542 - val_loss: 1.3295 - val_acc: 0.6305\n",
            "Epoch 8/10\n",
            "1686687/1686687 [==============================] - 131s 78us/step - loss: 1.1677 - acc: 0.6544 - val_loss: 1.3241 - val_acc: 0.6376\n",
            "Epoch 9/10\n",
            "1686687/1686687 [==============================] - 131s 78us/step - loss: 1.1683 - acc: 0.6543 - val_loss: 1.3243 - val_acc: 0.6312\n",
            "Epoch 10/10\n",
            "1686687/1686687 [==============================] - 132s 78us/step - loss: 1.1690 - acc: 0.6544 - val_loss: 1.3089 - val_acc: 0.6383\n",
            "completed training model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZsPY2hB8c_m",
        "colab_type": "text"
      },
      "source": [
        "# Use content specific Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o88f7ods8c_n",
        "colab_type": "text"
      },
      "source": [
        "##### bioasq\n",
        "\n",
        "\"We applied word2vec to a corpus of 10,876,004 English abstracts of biomedical articles from PubMed. The resulting vectors of 1,701,632 distinct words (types) are now publicly available from http://bioasq.lip6.fr/tools/BioASQword2vec/. File size: 1.3GB (compressed), 3.5GB (uncompressed).\"\n",
        "\n",
        "SOURCE: http://bioasq.org/news/bioasq-releases-continuous-space-word-vectors-obtained-applying-word2vec-pubmed-abstracts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH6CWvBCPMiS",
        "colab_type": "code",
        "outputId": "73dfa918-c3f6-4307-c99f-1a7a4e8ae00f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "!wget http://bioasq.lip6.fr/tools/BioASQword2vec  # download bio word embeddings\n",
        "!mv BioASQword2vec BioASQword2vec.tar.gz          # update the downloaded file to the correct .tag.gz name\n",
        "!tar -xvzf BioASQword2vec.tar.gz                  # unzip the file\n",
        "!ls"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-16 14:40:41--  http://bioasq.lip6.fr/tools/BioASQword2vec\n",
            "Resolving bioasq.lip6.fr (bioasq.lip6.fr)... 132.227.201.38\n",
            "Connecting to bioasq.lip6.fr (bioasq.lip6.fr)|132.227.201.38|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://participants-area.bioasq.org/tools/BioASQword2vec [following]\n",
            "--2019-07-16 14:40:41--  http://participants-area.bioasq.org/tools/BioASQword2vec\n",
            "Resolving participants-area.bioasq.org (participants-area.bioasq.org)... 143.233.226.90\n",
            "Connecting to participants-area.bioasq.org (participants-area.bioasq.org)|143.233.226.90|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://participants-area.bioasq.org/tools/BioASQword2vec/ [following]\n",
            "--2019-07-16 14:40:42--  http://participants-area.bioasq.org/tools/BioASQword2vec/\n",
            "Reusing existing connection to participants-area.bioasq.org:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1341997858 (1.2G) [application/force-download]\n",
            "Saving to: ‘BioASQword2vec’\n",
            "\n",
            "BioASQword2vec      100%[===================>]   1.25G  10.4MB/s    in 2m 4s   \n",
            "\n",
            "2019-07-16 14:42:45 (10.4 MB/s) - ‘BioASQword2vec’ saved [1341997858/1341997858]\n",
            "\n",
            "word2vecTools/toolkit.py\n",
            "word2vecTools/vectors.txt\n",
            "word2vecTools/\n",
            "word2vecTools/README_BioASQ_word_vectors.pdf\n",
            "word2vecTools/types.txt\n",
            "word2vecTools/train_vectors.sh\n",
            "BioASQword2vec.tar.gz\t glove.840B.300d.txt\t\t      word2vecTools\n",
            "exporter_train_data.csv  sample_data\n",
            "gdrive\t\t\t test_data_RePORTER_PRJ_C_FY2017.csv\n",
            "CPU times: user 1.81 s, sys: 825 ms, total: 2.64 s\n",
            "Wall time: 2min 47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjKwGD_Z8c_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BioasqEmbeddingModel(EmbeddingModel):\n",
        "\n",
        "    # override the EmbeddingModel's create_embeddings_index to read in bioasq embeddings\n",
        "    def create_embeddings_index(self):\n",
        "\n",
        "        # read in a file with all the learned tokens\n",
        "        with open(r'word2vecTools/types.txt', 'r') as f:\n",
        "            tokens = [line.strip() for line in f]\n",
        "\n",
        "        # read in a file with the associated embeddings for the tokens\n",
        "        with open('word2vecTools/vectors.txt', 'rb') as f:\n",
        "            embeddings = [np.asarray(embedding.split(), dtype='float32') for embedding in f]\n",
        "\n",
        "        # create a dict of the word --> embedding mappings\n",
        "        embeddings_index = {word:embedding for word, embedding in zip(tokens, embeddings)}\n",
        "\n",
        "        print('completed creating pubmed embedding index')\n",
        "        self.embeddings_index = embeddings_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "vFYOkpvP8c_n",
        "colab_type": "code",
        "outputId": "5254190c-5084-45d2-fbcc-3939b91111a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# set-up model pipeline\n",
        "bioasq_model = BioasqEmbeddingModel(\n",
        "    X_train=X_train, \n",
        "    X_test=X_test, \n",
        "    y_train=y_train,\n",
        "    y_test=y_test,\n",
        "    max_num_words=20000,\n",
        "    max_seq_length=75,\n",
        "    embedding_size=200\n",
        ")\n",
        "bioasq_model.setup_model_pipeline()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed tokenizing and padding text - train: True\n",
            "completed tokenizing and padding text - train: False\n",
            "completed creating pubmed embedding index\n",
            "completed creating embedding matrix\n",
            "model pipeline set-up complete\n",
            "CPU times: user 2min 4s, sys: 4.07 s, total: 2min 8s\n",
            "Wall time: 2min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMlcKC9p8c_o",
        "colab_type": "code",
        "outputId": "ddfe58bb-2111-4ad9-dac3-dd500824886e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# default model if a custom model is not provided\n",
        "model = Sequential()\n",
        "model.add(bioasq_model.get_embedding_layer())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(bioasq_model.num_classes, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train model\n",
        "bioasq_model.fit(epochs=10, model=model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using custom model\n",
            "Train on 1686687 samples, validate on 15341 samples\n",
            "Epoch 1/10\n",
            "1686687/1686687 [==============================] - 132s 78us/step - loss: 1.1349 - acc: 0.6568 - val_loss: 1.2286 - val_acc: 0.6389\n",
            "Epoch 2/10\n",
            "1686687/1686687 [==============================] - 132s 78us/step - loss: 1.1009 - acc: 0.6674 - val_loss: 1.2239 - val_acc: 0.6391\n",
            "Epoch 3/10\n",
            "1686687/1686687 [==============================] - 131s 78us/step - loss: 1.0994 - acc: 0.6684 - val_loss: 1.2165 - val_acc: 0.6418\n",
            "Epoch 4/10\n",
            "1686687/1686687 [==============================] - 132s 78us/step - loss: 1.0989 - acc: 0.6688 - val_loss: 1.2263 - val_acc: 0.6406\n",
            "Epoch 5/10\n",
            "1686687/1686687 [==============================] - 132s 78us/step - loss: 1.0986 - acc: 0.6691 - val_loss: 1.2231 - val_acc: 0.6435\n",
            "Epoch 6/10\n",
            "1686687/1686687 [==============================] - 131s 78us/step - loss: 1.0987 - acc: 0.6690 - val_loss: 1.2200 - val_acc: 0.6450\n",
            "Epoch 7/10\n",
            "1686687/1686687 [==============================] - 132s 78us/step - loss: 1.0987 - acc: 0.6689 - val_loss: 1.2272 - val_acc: 0.6451\n",
            "Epoch 8/10\n",
            "1686687/1686687 [==============================] - 131s 78us/step - loss: 1.0987 - acc: 0.6694 - val_loss: 1.2242 - val_acc: 0.6431\n",
            "Epoch 9/10\n",
            "1686687/1686687 [==============================] - 132s 78us/step - loss: 1.0987 - acc: 0.6695 - val_loss: 1.2291 - val_acc: 0.6442\n",
            "Epoch 10/10\n",
            "1686687/1686687 [==============================] - 131s 78us/step - loss: 1.0986 - acc: 0.6694 - val_loss: 1.2335 - val_acc: 0.6407\n",
            "completed training model\n",
            "CPU times: user 28min 2s, sys: 2min 39s, total: 30min 41s\n",
            "Wall time: 21min 57s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16dibFeM8c_p",
        "colab_type": "text"
      },
      "source": [
        "##### Biomedical natural language processing (Pubmed, PMC, and Wikipedia combined embeddings)\n",
        "\n",
        "NOTE: This embedding file is 4GB\n",
        "\n",
        "\"The openly available biomedical literature contains over 5 billion words in publication abstracts and full texts. Recent advances in unsupervised language processing methods have made it possible to make use of such large unannotated corpora for building statistical language models and inducing high quality vector space representations, which are, in turn, of utility in many tasks such as text classification, named entity recognition and query expansion. In this study, we introduce the first set of such language resources created from analysis of the entire available biomedical literature, including a dataset of all 1- to 5-grams and their probabilities in these texts and new models of word semantics. We discuss the opportunities created by these resources and demonstrate their application. All resources introduced in this study are available under open licenses at http://bio.nlplab.org.\"\n",
        "\n",
        "SOURCE: http://bio.nlplab.org/#word-vector-tools\n",
        "\n",
        "PUBLICATION: http://bio.nlplab.org/pdf/pyysalo13literature.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCaCWSWiIc2j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7415456a-2c1e-44ef-9bf9-3b09ef84041e"
      },
      "source": [
        "# The following code will only work after the embedding has been downloaded and saved\n",
        "# Copy embeddings from Google Drive to local Colab\n",
        "!cp gdrive/My\\ Drive/Colab\\ Notebooks/nih_data/wikipedia-pubmed-and-PMC-w2v.bin .\n",
        "!ls"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "exporter_train_data.csv  test_data_RePORTER_PRJ_C_FY2017.csv\n",
            "gdrive\t\t\t wikipedia-pubmed-and-PMC-w2v.bin\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMyVANsMXRdg",
        "colab_type": "code",
        "outputId": "72a60f54-10a5-4594-8923-41d4c1746e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# download bio.nlplab embeddings\n",
        "!wget http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-14 14:17:12--  http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin\n",
            "Resolving evexdb.org (evexdb.org)... 130.232.253.44\n",
            "Connecting to evexdb.org (evexdb.org)|130.232.253.44|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4416560851 (4.1G) [application/octet-stream]\n",
            "Saving to: ‘wikipedia-pubmed-and-PMC-w2v.bin’\n",
            "\n",
            "wikipedia-pubmed-an 100%[===================>]   4.11G  1.05MB/s    in 70m 32s \n",
            "\n",
            "2019-07-14 15:27:45 (1019 KB/s) - ‘wikipedia-pubmed-and-PMC-w2v.bin’ saved [4416560851/4416560851]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6bcP9_nnnJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OPTIONAL - move embeddings to personal Google Drive (to avoid large repeated download)\n",
        "!cp wikipedia-pubmed-and-PMC-w2v.bin gdrive/My\\ Drive/Colab\\ Notebooks/nih_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGaiIpmC8c_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PubmedEmbeddingModel(EmbeddingModel):\n",
        "\n",
        "    # override the EmbeddingModel's create_embeddings_index to read in pubmed embeddings\n",
        "    def create_embeddings_index(self):\n",
        "\n",
        "        embedding_path = r'wikipedia-pubmed-and-PMC-w2v.bin'\n",
        "        word_vectors = KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
        "\n",
        "        # create a dict of the word --> embedding mappings\n",
        "        embeddings_index = {word: word_vectors.get_vector(word) for word in word_vectors.index2word}\n",
        "\n",
        "        print('completed creating pubmed embedding index')\n",
        "        self.embeddings_index = embeddings_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsxOmrfo8c_q",
        "colab_type": "code",
        "outputId": "1ee6ca5f-9a31-4d9c-dd1c-29c1e1b8497d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# set-up model pipeline\n",
        "pubmed_model = PubmedEmbeddingModel(\n",
        "    X_train=X_train, \n",
        "    X_test=X_test, \n",
        "    y_train=y_train,\n",
        "    y_test=y_test,\n",
        "    max_num_words=25000,\n",
        "    max_seq_length=75,\n",
        "    embedding_size=200\n",
        ")\n",
        "pubmed_model.setup_model_pipeline()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed tokenizing and padding text - train: True\n",
            "completed tokenizing and padding text - train: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "completed creating pubmed embedding index\n",
            "completed creating embedding matrix\n",
            "model pipeline set-up complete\n",
            "CPU times: user 5min 52s, sys: 8.57 s, total: 6min\n",
            "Wall time: 5min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk4OoVc-8c_r",
        "colab_type": "code",
        "outputId": "8c5279d5-d39e-4f01-d3af-61c4f3abbfa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "pubmed_model.fit(epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0714 16:12:39.076960 140437015357312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0714 16:12:39.130147 140437015357312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0714 16:12:39.388870 140437015357312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0714 16:12:39.412070 140437015357312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0714 16:12:39.433096 140437015357312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0714 16:12:39.576600 140437015357312 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0714 16:12:39.632627 140437015357312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1686687 samples, validate on 15341 samples\n",
            "Epoch 1/10\n",
            "1686687/1686687 [==============================] - 150s 89us/step - loss: 1.1860 - acc: 0.6408 - val_loss: 1.2744 - val_acc: 0.6238\n",
            "Epoch 2/10\n",
            "1686687/1686687 [==============================] - 142s 84us/step - loss: 1.1502 - acc: 0.6522 - val_loss: 1.2786 - val_acc: 0.6258\n",
            "Epoch 3/10\n",
            "1686687/1686687 [==============================] - 137s 81us/step - loss: 1.1481 - acc: 0.6530 - val_loss: 1.2684 - val_acc: 0.6282\n",
            "Epoch 4/10\n",
            "1686687/1686687 [==============================] - 138s 82us/step - loss: 1.1475 - acc: 0.6537 - val_loss: 1.2702 - val_acc: 0.6297\n",
            "Epoch 5/10\n",
            "1686687/1686687 [==============================] - 137s 81us/step - loss: 1.1473 - acc: 0.6541 - val_loss: 1.2731 - val_acc: 0.6314\n",
            "Epoch 6/10\n",
            "1686687/1686687 [==============================] - 137s 81us/step - loss: 1.1475 - acc: 0.6541 - val_loss: 1.2704 - val_acc: 0.6308\n",
            "Epoch 7/10\n",
            "1686687/1686687 [==============================] - 137s 81us/step - loss: 1.1474 - acc: 0.6542 - val_loss: 1.2688 - val_acc: 0.6322\n",
            "Epoch 8/10\n",
            "1686687/1686687 [==============================] - 136s 81us/step - loss: 1.1472 - acc: 0.6546 - val_loss: 1.2728 - val_acc: 0.6294\n",
            "Epoch 9/10\n",
            "1686687/1686687 [==============================] - 140s 83us/step - loss: 1.1476 - acc: 0.6545 - val_loss: 1.2789 - val_acc: 0.6318\n",
            "Epoch 10/10\n",
            "1686687/1686687 [==============================] - 139s 82us/step - loss: 1.1472 - acc: 0.6545 - val_loss: 1.2877 - val_acc: 0.6286\n",
            "completed training model\n",
            "CPU times: user 29min 41s, sys: 2min 49s, total: 32min 31s\n",
            "Wall time: 23min 14s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6nms74C8c_s",
        "colab_type": "text"
      },
      "source": [
        "# 4. build a deep learning model (e.g. convolutional neural network), ending in a softmax output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFWUSHHi8c_s",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional Neural Networ (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8u8GDo_GhFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "\n",
        "def build_cnn_model(model):\n",
        "    # keras sequential creates models layer-by-layer \n",
        "    # doesn't create models that share layers or have multiple inputs/outputs\n",
        "    cnn_model = Sequential()\n",
        "    \n",
        "    # load the pretrained embedding into the model\n",
        "    cnn_model.add(model.get_embedding_layer())\n",
        "\n",
        "    # create a 1D (Conv1D) convolutional layer for text (2D is for images)\n",
        "    # filters: the number of features to extract from the text\n",
        "    # kernel_size: the window size (how many words to look at per feature)\n",
        "    cnn_model.add(Conv1D(filters=1024, kernel_size=6, activation='relu'))    \n",
        "    \n",
        "    # final pooling before dense layer\n",
        "    cnn_model.add(GlobalMaxPooling1D())\n",
        "    \n",
        "    # dense layers for a feedforward neural network\n",
        "    cnn_model.add(Dense(model.num_classes, activation='softmax'))\n",
        "    \n",
        "    # compile model to set the optimizer, loss, and metrics\n",
        "    cnn_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIHXv8D1LuvG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "42df6aea-9643-405c-862d-9b8f4859a961"
      },
      "source": [
        "# set-up and train model\n",
        "cnn_model = build_cnn_model(embedding_model)\n",
        "embedding_model.fit(epochs=15, model=cnn_model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using custom model\n",
            "Train on 1686687 samples, validate on 15341 samples\n",
            "Epoch 1/15\n",
            "1686687/1686687 [==============================] - 347s 206us/step - loss: 1.0193 - acc: 0.7400 - val_loss: 1.2917 - val_acc: 0.7595\n",
            "Epoch 2/15\n",
            "1686687/1686687 [==============================] - 345s 204us/step - loss: 0.9615 - acc: 0.8128 - val_loss: 1.3595 - val_acc: 0.8019\n",
            "Epoch 3/15\n",
            "1686687/1686687 [==============================] - 344s 204us/step - loss: 0.8952 - acc: 0.8497 - val_loss: 1.3584 - val_acc: 0.8252\n",
            "Epoch 4/15\n",
            "1686687/1686687 [==============================] - 343s 203us/step - loss: 0.8462 - acc: 0.8711 - val_loss: 1.3407 - val_acc: 0.8434\n",
            "Epoch 5/15\n",
            "1686687/1686687 [==============================] - 343s 204us/step - loss: 0.8107 - acc: 0.8849 - val_loss: 1.3463 - val_acc: 0.8487\n",
            "Epoch 6/15\n",
            "1686687/1686687 [==============================] - 343s 203us/step - loss: 0.7845 - acc: 0.8946 - val_loss: 1.2861 - val_acc: 0.8611\n",
            "Epoch 7/15\n",
            "1686687/1686687 [==============================] - 342s 203us/step - loss: 0.7617 - acc: 0.9016 - val_loss: 1.3169 - val_acc: 0.8657\n",
            "Epoch 8/15\n",
            "1686687/1686687 [==============================] - 343s 203us/step - loss: 0.7469 - acc: 0.9066 - val_loss: 1.3059 - val_acc: 0.8691\n",
            "Epoch 9/15\n",
            "1686687/1686687 [==============================] - 343s 203us/step - loss: 0.7346 - acc: 0.9106 - val_loss: 1.2730 - val_acc: 0.8735\n",
            "Epoch 10/15\n",
            "1686687/1686687 [==============================] - 342s 203us/step - loss: 0.7216 - acc: 0.9139 - val_loss: 1.2963 - val_acc: 0.8757\n",
            "Epoch 11/15\n",
            "1686687/1686687 [==============================] - 341s 202us/step - loss: 0.7141 - acc: 0.9167 - val_loss: 1.2648 - val_acc: 0.8775\n",
            "Epoch 12/15\n",
            "1686687/1686687 [==============================] - 341s 202us/step - loss: 0.7102 - acc: 0.9186 - val_loss: 1.2951 - val_acc: 0.8784\n",
            "Epoch 13/15\n",
            "1686687/1686687 [==============================] - 342s 203us/step - loss: 0.7071 - acc: 0.9208 - val_loss: 1.2599 - val_acc: 0.8815\n",
            "Epoch 14/15\n",
            "1686687/1686687 [==============================] - 342s 203us/step - loss: 0.6991 - acc: 0.9226 - val_loss: 1.2708 - val_acc: 0.8833\n",
            "Epoch 15/15\n",
            "1686687/1686687 [==============================] - 342s 203us/step - loss: 0.6948 - acc: 0.9238 - val_loss: 1.2479 - val_acc: 0.8847\n",
            "completed training model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU8hlImfNmYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "57a403c3-813c-4c45-dbc9-72b595b4c025"
      },
      "source": [
        "cnn_model = build_cnn_model(bioasq_model)\n",
        "bioasq_model.fit(epochs=20, model=cnn_model)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using custom model\n",
            "Train on 1686687 samples, validate on 15341 samples\n",
            "Epoch 1/20\n",
            "1686687/1686687 [==============================] - 326s 193us/step - loss: 0.7943 - acc: 0.7784 - val_loss: 0.8935 - val_acc: 0.7935\n",
            "Epoch 2/20\n",
            "1686687/1686687 [==============================] - 324s 192us/step - loss: 0.6271 - acc: 0.8571 - val_loss: 0.8460 - val_acc: 0.8478\n",
            "Epoch 3/20\n",
            "1686687/1686687 [==============================] - 323s 192us/step - loss: 0.5507 - acc: 0.8903 - val_loss: 0.8387 - val_acc: 0.8663\n",
            "Epoch 4/20\n",
            "1686687/1686687 [==============================] - 317s 188us/step - loss: 0.5110 - acc: 0.9079 - val_loss: 0.8585 - val_acc: 0.8800\n",
            "Epoch 5/20\n",
            "1686687/1686687 [==============================] - 318s 188us/step - loss: 0.4887 - acc: 0.9190 - val_loss: 0.8533 - val_acc: 0.8894\n",
            "Epoch 6/20\n",
            "1686687/1686687 [==============================] - 317s 188us/step - loss: 0.4769 - acc: 0.9263 - val_loss: 0.8642 - val_acc: 0.8940\n",
            "Epoch 7/20\n",
            "1686687/1686687 [==============================] - 315s 186us/step - loss: 0.4704 - acc: 0.9312 - val_loss: 0.8155 - val_acc: 0.9022\n",
            "Epoch 8/20\n",
            "1686687/1686687 [==============================] - 317s 188us/step - loss: 0.4688 - acc: 0.9349 - val_loss: 0.8939 - val_acc: 0.9025\n",
            "Epoch 9/20\n",
            "1686687/1686687 [==============================] - 316s 187us/step - loss: 0.4677 - acc: 0.9377 - val_loss: 0.8251 - val_acc: 0.9090\n",
            "Epoch 10/20\n",
            "1686687/1686687 [==============================] - 315s 187us/step - loss: 0.4648 - acc: 0.9400 - val_loss: 0.8730 - val_acc: 0.9077\n",
            "Epoch 11/20\n",
            "1686687/1686687 [==============================] - 314s 186us/step - loss: 0.4662 - acc: 0.9417 - val_loss: 0.8547 - val_acc: 0.9107\n",
            "Epoch 12/20\n",
            "1686687/1686687 [==============================] - 316s 187us/step - loss: 0.4658 - acc: 0.9433 - val_loss: 0.8526 - val_acc: 0.9117\n",
            "Epoch 13/20\n",
            "1686687/1686687 [==============================] - 315s 187us/step - loss: 0.4718 - acc: 0.9443 - val_loss: 0.9236 - val_acc: 0.9095\n",
            "Epoch 14/20\n",
            "1686687/1686687 [==============================] - 314s 186us/step - loss: 0.4731 - acc: 0.9452 - val_loss: 0.8780 - val_acc: 0.9144\n",
            "Epoch 15/20\n",
            "1686687/1686687 [==============================] - 314s 186us/step - loss: 0.4768 - acc: 0.9460 - val_loss: 0.8847 - val_acc: 0.9116\n",
            "Epoch 16/20\n",
            "1686687/1686687 [==============================] - 314s 186us/step - loss: 0.4762 - acc: 0.9471 - val_loss: 0.8807 - val_acc: 0.9155\n",
            "Epoch 17/20\n",
            "1686687/1686687 [==============================] - 314s 186us/step - loss: 0.4785 - acc: 0.9477 - val_loss: 0.9190 - val_acc: 0.9132\n",
            "Epoch 18/20\n",
            "1686687/1686687 [==============================] - 314s 186us/step - loss: 0.4833 - acc: 0.9483 - val_loss: 0.8994 - val_acc: 0.9140\n",
            "Epoch 19/20\n",
            "1686687/1686687 [==============================] - 314s 186us/step - loss: 0.4848 - acc: 0.9486 - val_loss: 0.9251 - val_acc: 0.9136\n",
            "Epoch 20/20\n",
            "1686687/1686687 [==============================] - 315s 187us/step - loss: 0.4847 - acc: 0.9492 - val_loss: 0.9320 - val_acc: 0.9153\n",
            "completed training model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UrXQCIi8c_y",
        "colab_type": "code",
        "outputId": "d606f874-fdab-4772-c9bd-d614cde6abbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "cnn_model = build_cnn_model(pubmed_model)\n",
        "pubmed_model.fit(epochs=15, model=cnn_model)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using custom model\n",
            "Train on 1686687 samples, validate on 15341 samples\n",
            "Epoch 1/15\n",
            "1686687/1686687 [==============================] - 302s 179us/step - loss: 0.8690 - acc: 0.7589 - val_loss: 1.0460 - val_acc: 0.7711\n",
            "Epoch 2/15\n",
            "1686687/1686687 [==============================] - 302s 179us/step - loss: 0.7463 - acc: 0.8320 - val_loss: 1.0250 - val_acc: 0.8198\n",
            "Epoch 3/15\n",
            "1686687/1686687 [==============================] - 302s 179us/step - loss: 0.6755 - acc: 0.8669 - val_loss: 0.9914 - val_acc: 0.8452\n",
            "Epoch 4/15\n",
            "1686687/1686687 [==============================] - 302s 179us/step - loss: 0.6281 - acc: 0.8878 - val_loss: 1.0342 - val_acc: 0.8531\n",
            "Epoch 5/15\n",
            "1686687/1686687 [==============================] - 302s 179us/step - loss: 0.5969 - acc: 0.9013 - val_loss: 1.0178 - val_acc: 0.8668\n",
            "Epoch 6/15\n",
            "1686687/1686687 [==============================] - 301s 179us/step - loss: 0.5771 - acc: 0.9105 - val_loss: 1.0197 - val_acc: 0.8764\n",
            "Epoch 7/15\n",
            "1686687/1686687 [==============================] - 301s 179us/step - loss: 0.5647 - acc: 0.9169 - val_loss: 1.0218 - val_acc: 0.8810\n",
            "Epoch 8/15\n",
            "1686687/1686687 [==============================] - 299s 177us/step - loss: 0.5531 - acc: 0.9220 - val_loss: 1.0088 - val_acc: 0.8891\n",
            "Epoch 9/15\n",
            "1686687/1686687 [==============================] - 299s 178us/step - loss: 0.5496 - acc: 0.9257 - val_loss: 1.0126 - val_acc: 0.8920\n",
            "Epoch 10/15\n",
            "1686687/1686687 [==============================] - 300s 178us/step - loss: 0.5435 - acc: 0.9288 - val_loss: 1.0132 - val_acc: 0.8937\n",
            "Epoch 11/15\n",
            " 268288/1686687 [===>..........................] - ETA: 4:10 - loss: 0.5110 - acc: 0.9341Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GR-RmWB8c_z",
        "colab_type": "text"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDD7qSW8AZnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "def build_lstm(model):\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(model.get_embedding_layer())\n",
        "    lstm_model.add(LSTM(32))\n",
        "    lstm_model.add(Dense(model.num_classes, activation='softmax'))\n",
        "    lstm_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return lstm_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFe53gBpcYgu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "ade68ab4-6a5e-410e-952f-f2a327b6d6d3"
      },
      "source": [
        "# set-up and train model\n",
        "lstm_model = build_lstm(embedding_model)\n",
        "embedding_model.fit(epochs=5, model=lstm_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using custom model\n",
            "Train on 1686687 samples, validate on 15341 samples\n",
            "Epoch 1/5\n",
            "1686687/1686687 [==============================] - 4710s 3ms/step - loss: 1.1542 - acc: 0.6542 - val_loss: 1.0463 - val_acc: 0.6881\n",
            "Epoch 2/5\n",
            " 150432/1686687 [=>............................] - ETA: 1:11:50 - loss: 0.9507 - acc: 0.7146Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApLUSaiS8c_z",
        "colab_type": "code",
        "outputId": "75efc2d4-9cbf-4718-8a19-ab3e20205e8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# set-up and train model\n",
        "lstm_model = build_lstm(bioasq_model)\n",
        "bioasq_model.fit(epochs=6, model=lstm_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using custom model\n",
            "Train on 1686687 samples, validate on 15341 samples\n",
            "Epoch 1/6\n",
            "1686687/1686687 [==============================] - 3137s 2ms/step - loss: 1.3710 - acc: 0.5768 - val_loss: 1.2039 - val_acc: 0.6520\n",
            "Epoch 2/6\n",
            "1686687/1686687 [==============================] - 3101s 2ms/step - loss: 1.0081 - acc: 0.7081 - val_loss: 1.0776 - val_acc: 0.6822\n",
            "Epoch 3/6\n",
            "1686687/1686687 [==============================] - 3116s 2ms/step - loss: 0.9236 - acc: 0.7264 - val_loss: 1.0086 - val_acc: 0.7018\n",
            "Epoch 4/6\n",
            "1686687/1686687 [==============================] - 3122s 2ms/step - loss: 0.8878 - acc: 0.7351 - val_loss: 0.9999 - val_acc: 0.7061\n",
            "Epoch 5/6\n",
            "1686687/1686687 [==============================] - 3122s 2ms/step - loss: 0.8671 - acc: 0.7401 - val_loss: 0.9834 - val_acc: 0.7075\n",
            "Epoch 6/6\n",
            "1686687/1686687 [==============================] - 3114s 2ms/step - loss: 0.8526 - acc: 0.7439 - val_loss: 0.9718 - val_acc: 0.7090\n",
            "completed training model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2SjsNFBVoC5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "d8e5f7d6-6e05-4942-ff14-e49dda32d9c0"
      },
      "source": [
        "lstm_model = build_lstm(pubmed_model)\n",
        "pubmed_model.fit(epochs=10, model=lstm_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0714 20:27:45.688020 140011109181312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0714 20:27:45.776172 140011109181312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0714 20:27:46.110307 140011109181312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0714 20:27:46.404247 140011109181312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0714 20:27:46.430250 140011109181312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0714 20:27:46.613351 140011109181312 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using custom model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0714 20:27:47.276360 140011109181312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1686687 samples, validate on 15341 samples\n",
            "Epoch 1/10\n",
            "1686687/1686687 [==============================] - 3058s 2ms/step - loss: 1.1844 - acc: 0.6411 - val_loss: 1.0969 - val_acc: 0.6765\n",
            "Epoch 2/10\n",
            " 153632/1686687 [=>............................] - ETA: 46:39 - loss: 0.9787 - acc: 0.7063Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5otksp48c_2",
        "colab_type": "code",
        "outputId": "aadf6301-6ff5-4f4c-c0ff-bede0b0e1401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        }
      },
      "source": [
        "lstm_model = build_lstm(pubmed_model)\n",
        "pubmed_model.fit(epochs=25, model=lstm_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using custom model\n",
            "Train on 46022 samples, validate on 15341 samples\n",
            "Epoch 1/25\n",
            "46022/46022 [==============================] - 173s 4ms/step - loss: 2.0310 - acc: 0.2950 - val_loss: 1.9802 - val_acc: 0.3540\n",
            "Epoch 2/25\n",
            "46022/46022 [==============================] - 172s 4ms/step - loss: 1.8118 - acc: 0.4102 - val_loss: 1.7403 - val_acc: 0.4432\n",
            "Epoch 3/25\n",
            "46022/46022 [==============================] - 171s 4ms/step - loss: 1.6604 - acc: 0.4674 - val_loss: 1.5735 - val_acc: 0.4947\n",
            "Epoch 4/25\n",
            "46022/46022 [==============================] - 172s 4ms/step - loss: 1.5358 - acc: 0.5038 - val_loss: 1.4728 - val_acc: 0.5247\n",
            "Epoch 5/25\n",
            "46022/46022 [==============================] - 171s 4ms/step - loss: 1.3421 - acc: 0.5877 - val_loss: 1.3001 - val_acc: 0.6089\n",
            "Epoch 6/25\n",
            "46022/46022 [==============================] - 171s 4ms/step - loss: 1.2130 - acc: 0.6387 - val_loss: 1.2370 - val_acc: 0.6284\n",
            "Epoch 7/25\n",
            "46022/46022 [==============================] - 171s 4ms/step - loss: 1.1385 - acc: 0.6598 - val_loss: 1.1751 - val_acc: 0.6485\n",
            "Epoch 8/25\n",
            "46022/46022 [==============================] - 172s 4ms/step - loss: 1.0798 - acc: 0.6753 - val_loss: 1.1481 - val_acc: 0.6535\n",
            "Epoch 9/25\n",
            "46022/46022 [==============================] - 171s 4ms/step - loss: 1.0315 - acc: 0.6894 - val_loss: 1.1293 - val_acc: 0.6620\n",
            "Epoch 10/25\n",
            "46022/46022 [==============================] - 172s 4ms/step - loss: 0.9922 - acc: 0.6992 - val_loss: 1.1405 - val_acc: 0.6623\n",
            "Epoch 11/25\n",
            "46022/46022 [==============================] - 172s 4ms/step - loss: 0.9595 - acc: 0.7096 - val_loss: 1.0874 - val_acc: 0.6741\n",
            "Epoch 12/25\n",
            "46022/46022 [==============================] - 172s 4ms/step - loss: 0.9268 - acc: 0.7189 - val_loss: 1.0759 - val_acc: 0.6771\n",
            "Epoch 13/25\n",
            "46022/46022 [==============================] - 178s 4ms/step - loss: 0.8964 - acc: 0.7277 - val_loss: 1.0938 - val_acc: 0.6765\n",
            "Epoch 14/25\n",
            "46022/46022 [==============================] - 173s 4ms/step - loss: 0.8713 - acc: 0.7364 - val_loss: 1.0837 - val_acc: 0.6816\n",
            "Epoch 15/25\n",
            "46022/46022 [==============================] - 174s 4ms/step - loss: 0.8471 - acc: 0.7428 - val_loss: 1.1109 - val_acc: 0.6772\n",
            "Epoch 16/25\n",
            "46022/46022 [==============================] - 177s 4ms/step - loss: 0.8255 - acc: 0.7497 - val_loss: 1.0866 - val_acc: 0.6852\n",
            "Epoch 17/25\n",
            "46022/46022 [==============================] - 179s 4ms/step - loss: 0.8037 - acc: 0.7547 - val_loss: 1.1229 - val_acc: 0.6827\n",
            "Epoch 18/25\n",
            "46022/46022 [==============================] - 174s 4ms/step - loss: 0.7796 - acc: 0.7626 - val_loss: 1.0976 - val_acc: 0.6867\n",
            "Epoch 19/25\n",
            "46022/46022 [==============================] - 179s 4ms/step - loss: 0.7638 - acc: 0.7681 - val_loss: 1.0976 - val_acc: 0.6867\n",
            "Epoch 20/25\n",
            "46022/46022 [==============================] - 179s 4ms/step - loss: 0.7446 - acc: 0.7738 - val_loss: 1.1149 - val_acc: 0.6850\n",
            "Epoch 21/25\n",
            "46022/46022 [==============================] - 176s 4ms/step - loss: 0.7264 - acc: 0.7794 - val_loss: 1.1168 - val_acc: 0.6829\n",
            "Epoch 22/25\n",
            "46022/46022 [==============================] - 176s 4ms/step - loss: 0.7070 - acc: 0.7849 - val_loss: 1.1317 - val_acc: 0.6859\n",
            "Epoch 23/25\n",
            "46022/46022 [==============================] - 176s 4ms/step - loss: 0.6887 - acc: 0.7918 - val_loss: 1.1481 - val_acc: 0.6846\n",
            "Epoch 24/25\n",
            "46022/46022 [==============================] - 177s 4ms/step - loss: 0.6728 - acc: 0.7953 - val_loss: 1.1632 - val_acc: 0.6863\n",
            "Epoch 25/25\n",
            "46022/46022 [==============================] - 175s 4ms/step - loss: 0.6586 - acc: 0.8003 - val_loss: 1.2146 - val_acc: 0.6752\n",
            "completed training model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqCQqx528c_7",
        "colab_type": "text"
      },
      "source": [
        "# FLAIR \n",
        "\n",
        "#### NOTE: Due to Colab 12GB memory constraint, you should probably reset your environment before running the below code or it will likely crash"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM1spNNq8c_8",
        "colab_type": "code",
        "outputId": "7966afb7-81fc-4954-86fa-53139fd1eb13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# install FLAIR\n",
        "\n",
        "!pip3 install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install https://download.pytorch.org/whl/cpu/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install flair\n",
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.1.0 from https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.16.4)\n",
            "Requirement already satisfied: torchvision==0.3.0 from https://download.pytorch.org/whl/cpu/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.12.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.1.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (4.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.16.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.3.0) (0.46)\n",
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/3a/2e777f65a71c1eaa259df44c44e39d7071ba8c7780a1564316a38bf86449/flair-0.4.2-py3-none-any.whl (136kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 3.3MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.0.3)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Collecting mpld3==0.3 (from flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.1.0)\n",
            "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Collecting deprecated>=1.2.4 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/88/0e/9d5a1a8cd7130c49334cce7b8167ceda63d6a329c8ea65b626116bc9e9e6/Deprecated-1.2.6-py2.py3-none-any.whl\n",
            "Collecting regex (from flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 50.8MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Collecting pytorch-pretrained-bert>=0.6.1 (from flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 52.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.3)\n",
            "Collecting bpemb>=0.2.9 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.16.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.5.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.8.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.21.2)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (7.1.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (41.0.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.1.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.8.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (1.9.180)\n",
            "Collecting sentencepiece (from bpemb>=0.2.9->flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->flair) (0.13.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (2019.6.16)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.180 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (1.12.180)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.9.4)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.180->boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.14)\n",
            "Building wheels for collected packages: sqlitedict, mpld3, regex, segtok\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
            "Successfully built sqlitedict mpld3 regex segtok\n",
            "Installing collected packages: sqlitedict, mpld3, deprecated, regex, segtok, pytorch-pretrained-bert, sentencepiece, bpemb, flair\n",
            "Successfully installed bpemb-0.3.0 deprecated-1.2.6 flair-0.4.2 mpld3-0.3 pytorch-pretrained-bert-0.6.2 regex-2019.6.8 segtok-1.5.7 sentencepiece-0.1.82 sqlitedict-1.6.0\n",
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/8c/72b14d20c9cbb0306939ea41109fc599302634fd5c59ccba1a659b7d0360/allennlp-0.8.4-py3-none-any.whl (5.7MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7MB 3.4MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 55.8MB/s \n",
            "\u001b[?25hCollecting unidecode (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 56.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\" (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/a8/adba6cd0f84ee6ab064e7f70cd03a2836cefd2e063fd565180ec13beae93/jsonnet-0.13.0.tar.gz (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 58.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Collecting jsonpickle (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Collecting flask-cors>=3.0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.3)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Collecting ftfy (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 13.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Collecting awscli>=1.11.91 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/c8/31819843ec82c0fb88b8a318d21b0164f30f18ac7799dbee7b7add1b324a/awscli-1.16.195-py2.py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 48.4MB/s \n",
            "\u001b[?25hCollecting conllu==0.11 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\n",
            "Collecting overrides (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.6.2)\n",
            "Collecting flaky (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/09/94d623dda1adacd51722f3e3e0f88ba08dd030ac2b2662bfb4383096340d/flaky-3.6.0-py2.py3-none-any.whl\n",
            "Collecting numpydoc>=0.8.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/f3/7cfe4c616e4b9fe05540256cc9c6661c052c8a4cec2915732793b36e1843/numpydoc-0.9.1.tar.gz\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.3)\n",
            "Collecting parsimonious>=0.8.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 23.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.16.4)\n",
            "Requirement already satisfied: spacy<2.2,>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.4)\n",
            "Collecting responses>=0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/5a/b887e89925f1de7890ef298a74438371ed4ed29b33def9e6d02dc6036fd8/responses-0.10.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Collecting word2number>=1.1 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.180)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.7.1)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.14)\n",
            "Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 25.2MB/s \n",
            "\u001b[?25hCollecting botocore==1.12.185 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/5c/cb7545729b23410b51ad3f64c4c21d92d9f48f0a7ff5027c2d20bd8c298b/botocore-1.12.185-py2.py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 47.3MB/s \n",
            "\u001b[?25hCollecting colorama<=0.3.9,>=0.2.5 (from awscli>=1.11.91->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/db/c8/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf/colorama-0.3.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.2.1)\n",
            "Requirement already satisfied: PyYAML<=5.1,>=3.10; python_version != \"2.6\" in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.6.8)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.10.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (7.1.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (41.0.1)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.15.4)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (1.0.2)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.6.0)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.2.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.2.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (7.0.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.9.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.13.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.9.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Building wheels for collected packages: jsonnet, overrides, numpydoc, parsimonious, word2number\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/30/ab/ae4a57b1df44fa20a531edb9601b27603da8f5336225691f3f\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/30/d1/92a39ba40f21cb70e53f8af96eb98f002a781843c065406500\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "Successfully built jsonnet overrides numpydoc parsimonious word2number\n",
            "Installing collected packages: tensorboardX, unidecode, jsonnet, jsonpickle, flask-cors, ftfy, rsa, botocore, colorama, awscli, conllu, overrides, flaky, numpydoc, parsimonious, responses, word2number, allennlp\n",
            "  Found existing installation: rsa 4.0\n",
            "    Uninstalling rsa-4.0:\n",
            "      Successfully uninstalled rsa-4.0\n",
            "  Found existing installation: botocore 1.12.180\n",
            "    Uninstalling botocore-1.12.180:\n",
            "      Successfully uninstalled botocore-1.12.180\n",
            "Successfully installed allennlp-0.8.4 awscli-1.16.195 botocore-1.12.185 colorama-0.3.9 conllu-0.11 flaky-3.6.0 flask-cors-3.0.8 ftfy-5.5.1 jsonnet-0.13.0 jsonpickle-1.2 numpydoc-0.9.1 overrides-1.9 parsimonious-0.8.1 responses-0.10.6 rsa-3.4.2 tensorboardX-1.8 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "botocore",
                  "rsa"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_lPSRZJC-IM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import flair\n",
        "from flair.data import Corpus\n",
        "from flair.datasets import ClassificationCorpus\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, ELMoEmbeddings, TransformerXLEmbeddings, DocumentRNNEmbeddings\n",
        "import pandas as pd\n",
        "\n",
        "# directory to download pretrained models\n",
        "flair.cache_root = '/tmp/embeddings'\n",
        "\n",
        "# this is the directory in which the train, test and dev files reside\n",
        "!mkdir flair_data\n",
        "data_folder = 'flair_data'\n",
        "train = 'flair_data/train.txt'\n",
        "test = 'flair_data/test.txt'\n",
        "dev = 'flair_data/dev.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEiGhXlttMlx",
        "colab_type": "code",
        "outputId": "fb60a7fb-525e-43df-ca46-5d098552daa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# separate the features and response\n",
        "X_train = df['PROJECT_TITLE'].apply(lambda x: preprocess_text(x))\n",
        "y_train = df['IC_NUM']\n",
        "X_test = test_df['PROJECT_TITLE'].apply(lambda x: preprocess_text(x))\n",
        "y_test = test_df['IC_NUM']\n",
        "\n",
        "# get a count of the number of possible categories to predict\n",
        "num_classes = len(set(y_train))\n",
        "\n",
        "# convert the training and testing dataset\n",
        "y_train_array = to_categorical(y_train, num_classes)\n",
        "y_test_array = to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 16.9 s, sys: 206 ms, total: 17.1 s\n",
            "Wall time: 17.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax0145UnIpUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_data_for_flair(X, y, filename):\n",
        "  \"\"\" create a .txt file with data formatted for flair (i.e. fasttext format) \"\"\"\n",
        "  \n",
        "  # combine X and y into a single dataframe\n",
        "  flair_df = pd.DataFrame(list(zip(X, y)), columns=['X','y'])\n",
        "  \n",
        "  # format data for flair (e.g. __label__<1> text)\n",
        "  formatted_data = [f\"__label__<{str(y)}> \" + x for x, y in flair_df[['X','y']].values]\n",
        "  \n",
        "  # write to local txt file\n",
        "  with open(filename, 'w+') as f:\n",
        "      # write data\n",
        "      for line in formatted_data[:-1]:\n",
        "          # skip empty lines\n",
        "          if len(line.strip().split(' ')) < 2:\n",
        "             continue\n",
        "          f.write(line + '\\n')\n",
        "      # do not add a '\\n' for the last line\n",
        "      f.write(formatted_data[-1])\n",
        "          \n",
        "format_data_for_flair(X_train, y_train, train)\n",
        "format_data_for_flair(X_test, y_test, test)\n",
        "format_data_for_flair(X_test, y_test, dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_rLCtYRRRA8",
        "colab_type": "code",
        "outputId": "f7d54120-3083-44a1-8377-7f6d75eda805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!head -n5 flair_data/train.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__label__<10> hiv and other infectious consequences of substance abuse\n",
            "__label__<0> predictive coding as framework for understanding psychosis\n",
            "__label__<6> the role of the gut microbiome host in heart failure related insulin resistance\n",
            "__label__<11> liver resident memory for malaria\n",
            "__label__<11> novel biomolecular and biophysical mechanisms of influenza virus infection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OtZi3jCLI1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy data to google drive\n",
        "# NOTE - you will need to create these folders (e.g. nih_data) in google drive for this to work\n",
        "\n",
        "!cp flair_train.txt gdrive/My\\ Drive/Colab\\ Notebooks/nih_data/train.txt\n",
        "!cp flair_test.txt gdrive/My\\ Drive/Colab\\ Notebooks/nih_data/test.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q-dvXn1Bk8G",
        "colab_type": "text"
      },
      "source": [
        "## Download Data formatted for FLAIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lTrmTw_LgJC",
        "colab_type": "text"
      },
      "source": [
        "# FLAIR NLP Modeling Pipeline\n",
        "\n",
        "1. load corpus containing training, test and dev data\n",
        "2. create the label dictionary\n",
        "3. make a list of word embeddings\n",
        "4. initialize document embedding by passing list of word embeddings\n",
        "5. create the text classifier\n",
        "6. initialize the text classifier trainer\n",
        "7. start the training\n",
        "8. plot training curves (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIUVhqIGDEK2",
        "colab_type": "code",
        "outputId": "c09af2aa-53ef-4797-ea26-9ad3acc6d7ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# load corpus containing training, test and dev data\n",
        "corpus = ClassificationCorpus(data_folder)\n",
        "corpus"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-10 00:38:20,322 Reading data from flair_data\n",
            "2019-07-10 00:38:20,323 Train: flair_data/train.txt\n",
            "2019-07-10 00:38:20,324 Dev: flair_data/dev.txt\n",
            "2019-07-10 00:38:20,325 Test: flair_data/test.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<flair.datasets.ClassificationCorpus at 0x7f812a0e5d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdr4YI-GDB-w",
        "colab_type": "code",
        "outputId": "fab293f5-8e6c-4239-87cd-11f99f2c0bdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# create the label dictionary\n",
        "label_dict = corpus.make_label_dictionary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-10 00:43:11,255 {'<1>', '<5>', '<8>', '<10>', '<11>', '<4>', '<7>', '<0>', '<2>', '<3>', '<6>', '<12>', '<9>'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Xsw05zZEZMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flair_model(word_embeddings, hidden_size=512, learning_rate=0.1, max_epochs=5):\n",
        "    \"\"\" train a classifier in FLAIR\n",
        "    :param word_embeddings: selected word embeddings to use in model\n",
        "    :param hidden_size: size of hidden layer\n",
        "    :param learning rate: model learning rate\n",
        "    :param max_epochs: epochs to train model\n",
        "    \"\"\"\n",
        "  \n",
        "    # model hyperparams\n",
        "    print(f'MODEL METADATA:\\n    hidden_size={hidden_size} | learning_rate={learning_rate} | max_epochs={max_epochs}\\n\\n')\n",
        "    \n",
        "    # initialize document embedding by passing list of word embeddings\n",
        "    # Can choose between many RNN types \n",
        "    # (GRU by default, to change use rnn_type parameter)\n",
        "    document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(\n",
        "        word_embeddings,\n",
        "        hidden_size=hidden_size,\n",
        "        reproject_words=True,\n",
        "        reproject_words_dimension=256\n",
        "    )\n",
        "\n",
        "    # create the text classifier\n",
        "    classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
        "\n",
        "    # initialize the text classifier trainer\n",
        "    trainer = ModelTrainer(classifier, corpus)\n",
        "\n",
        "    # start the training\n",
        "    trainer.train(\n",
        "        'flair',\n",
        "        learning_rate=learning_rate,\n",
        "        mini_batch_size=32,\n",
        "        max_epochs=max_epochs\n",
        "    )\n",
        "\n",
        "    # plot training curves (optional)\n",
        "    from flair.visual.training_curves import Plotter\n",
        "    plotter = Plotter()\n",
        "    plotter.plot_training_curves('flair/loss.tsv')\n",
        "    plotter.plot_weights('flair/weights.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joo81aKhEDgu",
        "colab_type": "code",
        "outputId": "8351be8b-ea0f-4d15-e2e2-fb5240403eeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# make a list of word embeddings\n",
        "word_embeddings = [WordEmbeddings('glove')]\n",
        "flair_model(word_embeddings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-10 00:47:44,743 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmposs83xtv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 160000128/160000128 [00:08<00:00, 19432496.42B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-07-10 00:47:53,468 copying /tmp/tmposs83xtv to cache at /tmp/embeddings/embeddings/glove.gensim.vectors.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-07-10 00:47:53,769 removing temp file /tmp/tmposs83xtv\n",
            "2019-07-10 00:47:54,259 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim not found in cache, downloading to /tmp/tmpgcux1wgf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21494764/21494764 [00:01<00:00, 12446714.67B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-07-10 00:47:56,459 copying /tmp/tmpgcux1wgf to cache at /tmp/embeddings/embeddings/glove.gensim\n",
            "2019-07-10 00:47:56,487 removing temp file /tmp/tmpgcux1wgf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MODEL METADATA:\n",
            "    hidden_size=512 | learning_rate=0.1 | max_epochs=5\n",
            "\n",
            "\n",
            "2019-07-10 00:48:05,507 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 00:48:05,508 Evaluation method: MICRO_F1_SCORE\n",
            "2019-07-10 00:48:05,922 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 00:48:06,652 epoch 1 - iter 0/52697 - loss 2.70994735\n",
            "2019-07-10 00:50:05,564 epoch 1 - iter 5269/52697 - loss 1.54137151\n",
            "2019-07-10 00:52:04,626 epoch 1 - iter 10538/52697 - loss 1.44024168\n",
            "2019-07-10 00:54:02,436 epoch 1 - iter 15807/52697 - loss 1.39074753\n",
            "2019-07-10 00:55:59,797 epoch 1 - iter 21076/52697 - loss 1.35826660\n",
            "2019-07-10 00:58:00,169 epoch 1 - iter 26345/52697 - loss 1.33330112\n",
            "2019-07-10 00:59:57,729 epoch 1 - iter 31614/52697 - loss 1.31519084\n",
            "2019-07-10 01:01:54,970 epoch 1 - iter 36883/52697 - loss 1.30087566\n",
            "2019-07-10 01:03:52,057 epoch 1 - iter 42152/52697 - loss 1.28735382\n",
            "2019-07-10 01:05:48,834 epoch 1 - iter 47421/52697 - loss 1.27601426\n",
            "2019-07-10 01:07:46,187 epoch 1 - iter 52690/52697 - loss 1.26648393\n",
            "2019-07-10 01:07:46,552 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 01:07:46,557 EPOCH 1 done: loss 1.2665 - lr 0.1000 - bad epochs 0\n",
            "2019-07-10 01:07:56,927 DEV : loss 1.1148834228515625 - score 0.6593\n",
            "2019-07-10 01:08:07,305 TEST : loss 1.1148834228515625 - score 0.6593\n",
            "2019-07-10 01:08:10,668 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 01:08:11,414 epoch 2 - iter 0/52697 - loss 1.00892031\n",
            "2019-07-10 01:10:09,710 epoch 2 - iter 5269/52697 - loss 1.17120602\n",
            "2019-07-10 01:12:08,350 epoch 2 - iter 10538/52697 - loss 1.16600412\n",
            "2019-07-10 01:14:06,919 epoch 2 - iter 15807/52697 - loss 1.16315548\n",
            "2019-07-10 01:16:04,803 epoch 2 - iter 21076/52697 - loss 1.16130060\n",
            "2019-07-10 01:18:02,775 epoch 2 - iter 26345/52697 - loss 1.16061884\n",
            "2019-07-10 01:20:00,456 epoch 2 - iter 31614/52697 - loss 1.15908439\n",
            "2019-07-10 01:22:01,519 epoch 2 - iter 36883/52697 - loss 1.15696589\n",
            "2019-07-10 01:23:59,781 epoch 2 - iter 42152/52697 - loss 1.15492680\n",
            "2019-07-10 01:25:57,520 epoch 2 - iter 47421/52697 - loss 1.15336670\n",
            "2019-07-10 01:27:55,841 epoch 2 - iter 52690/52697 - loss 1.15088623\n",
            "2019-07-10 01:27:56,242 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 01:27:56,244 EPOCH 2 done: loss 1.1509 - lr 0.1000 - bad epochs 0\n",
            "2019-07-10 01:28:06,672 DEV : loss 1.0593401193618774 - score 0.6733\n",
            "2019-07-10 01:28:17,061 TEST : loss 1.0593401193618774 - score 0.6733\n",
            "2019-07-10 01:28:20,378 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 01:28:21,226 epoch 3 - iter 0/52697 - loss 1.18531096\n",
            "2019-07-10 01:30:19,748 epoch 3 - iter 5269/52697 - loss 1.13301667\n",
            "2019-07-10 01:32:17,880 epoch 3 - iter 10538/52697 - loss 1.13038508\n",
            "2019-07-10 01:34:15,331 epoch 3 - iter 15807/52697 - loss 1.12983650\n",
            "2019-07-10 01:36:12,631 epoch 3 - iter 21076/52697 - loss 1.12931442\n",
            "2019-07-10 01:38:10,186 epoch 3 - iter 26345/52697 - loss 1.12768294\n",
            "2019-07-10 01:40:07,541 epoch 3 - iter 31614/52697 - loss 1.12603355\n",
            "2019-07-10 01:42:07,150 epoch 3 - iter 36883/52697 - loss 1.12428262\n",
            "2019-07-10 01:44:04,839 epoch 3 - iter 42152/52697 - loss 1.12302507\n",
            "2019-07-10 01:46:02,529 epoch 3 - iter 47421/52697 - loss 1.12185531\n",
            "2019-07-10 01:47:59,769 epoch 3 - iter 52690/52697 - loss 1.12129084\n",
            "2019-07-10 01:48:00,189 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 01:48:00,190 EPOCH 3 done: loss 1.1213 - lr 0.1000 - bad epochs 0\n",
            "2019-07-10 01:48:10,761 DEV : loss 1.0381336212158203 - score 0.6819\n",
            "2019-07-10 01:48:20,716 TEST : loss 1.0381336212158203 - score 0.6819\n",
            "2019-07-10 01:48:24,116 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 01:48:24,841 epoch 4 - iter 0/52697 - loss 1.22720671\n",
            "2019-07-10 01:50:26,140 epoch 4 - iter 5269/52697 - loss 1.11359169\n",
            "2019-07-10 01:52:24,451 epoch 4 - iter 10538/52697 - loss 1.10868970\n",
            "2019-07-10 01:54:21,552 epoch 4 - iter 15807/52697 - loss 1.10787519\n",
            "2019-07-10 01:56:18,965 epoch 4 - iter 21076/52697 - loss 1.10732197\n",
            "2019-07-10 01:58:16,223 epoch 4 - iter 26345/52697 - loss 1.10612387\n",
            "2019-07-10 02:00:13,776 epoch 4 - iter 31614/52697 - loss 1.10462787\n",
            "2019-07-10 02:02:11,005 epoch 4 - iter 36883/52697 - loss 1.10296971\n",
            "2019-07-10 02:04:11,817 epoch 4 - iter 42152/52697 - loss 1.10257438\n",
            "2019-07-10 02:06:09,048 epoch 4 - iter 47421/52697 - loss 1.10197613\n",
            "2019-07-10 02:08:06,282 epoch 4 - iter 52690/52697 - loss 1.10189819\n",
            "2019-07-10 02:08:06,702 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 02:08:06,705 EPOCH 4 done: loss 1.1019 - lr 0.1000 - bad epochs 0\n",
            "2019-07-10 02:08:17,073 DEV : loss 1.0314267873764038 - score 0.6877\n",
            "2019-07-10 02:08:27,374 TEST : loss 1.0314267873764038 - score 0.6877\n",
            "2019-07-10 02:08:30,682 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 02:08:31,443 epoch 5 - iter 0/52697 - loss 1.37792599\n",
            "2019-07-10 02:10:29,965 epoch 5 - iter 5269/52697 - loss 1.09316328\n",
            "2019-07-10 02:12:27,563 epoch 5 - iter 10538/52697 - loss 1.09296258\n",
            "2019-07-10 02:14:25,307 epoch 5 - iter 15807/52697 - loss 1.09065251\n",
            "2019-07-10 02:16:26,698 epoch 5 - iter 21076/52697 - loss 1.09031218\n",
            "2019-07-10 02:18:23,526 epoch 5 - iter 26345/52697 - loss 1.09088863\n",
            "2019-07-10 02:20:21,775 epoch 5 - iter 31614/52697 - loss 1.09063155\n",
            "2019-07-10 02:22:18,603 epoch 5 - iter 36883/52697 - loss 1.08978386\n",
            "2019-07-10 02:24:16,312 epoch 5 - iter 42152/52697 - loss 1.08895896\n",
            "2019-07-10 02:26:13,967 epoch 5 - iter 47421/52697 - loss 1.08882773\n",
            "2019-07-10 02:28:13,005 epoch 5 - iter 52690/52697 - loss 1.08876154\n",
            "2019-07-10 02:28:13,425 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 02:28:13,429 EPOCH 5 done: loss 1.0887 - lr 0.1000 - bad epochs 0\n",
            "2019-07-10 02:28:23,775 DEV : loss 0.992414116859436 - score 0.696\n",
            "2019-07-10 02:28:34,109 TEST : loss 0.992414116859436 - score 0.696\n",
            "2019-07-10 02:28:40,705 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 02:28:40,706 Testing using best model ...\n",
            "2019-07-10 02:28:40,711 loading file flair/best-model.pt\n",
            "2019-07-10 02:28:52,270 0.696\t0.696\t0.696\n",
            "2019-07-10 02:28:52,271 \n",
            "MICRO_AVG: acc 0.5337 - f1-score 0.696\n",
            "MACRO_AVG: acc 0.5308 - f1-score 0.6869461538461538\n",
            "<0>        tp: 560 - fp: 303 - fn: 319 - tn: 14155 - precision: 0.6489 - recall: 0.6371 - accuracy: 0.4738 - f1-score: 0.6429\n",
            "<10>       tp: 445 - fp: 64 - fn: 197 - tn: 14631 - precision: 0.8743 - recall: 0.6931 - accuracy: 0.6303 - f1-score: 0.7732\n",
            "<11>       tp: 1450 - fp: 572 - fn: 486 - tn: 12829 - precision: 0.7171 - recall: 0.7490 - accuracy: 0.5781 - f1-score: 0.7327\n",
            "<12>       tp: 243 - fp: 101 - fn: 151 - tn: 14842 - precision: 0.7064 - recall: 0.6168 - accuracy: 0.4909 - f1-score: 0.6586\n",
            "<1>        tp: 2441 - fp: 1184 - fn: 410 - tn: 11302 - precision: 0.6734 - recall: 0.8562 - accuracy: 0.6050 - f1-score: 0.7539\n",
            "<2>        tp: 276 - fp: 79 - fn: 163 - tn: 14819 - precision: 0.7775 - recall: 0.6287 - accuracy: 0.5328 - f1-score: 0.6952\n",
            "<3>        tp: 1217 - fp: 802 - fn: 679 - tn: 12639 - precision: 0.6028 - recall: 0.6419 - accuracy: 0.4511 - f1-score: 0.6217\n",
            "<4>        tp: 815 - fp: 575 - fn: 333 - tn: 13614 - precision: 0.5863 - recall: 0.7099 - accuracy: 0.4730 - f1-score: 0.6422\n",
            "<5>        tp: 694 - fp: 209 - fn: 288 - tn: 14146 - precision: 0.7685 - recall: 0.7067 - accuracy: 0.5827 - f1-score: 0.7363\n",
            "<6>        tp: 1108 - fp: 323 - fn: 506 - tn: 13400 - precision: 0.7743 - recall: 0.6865 - accuracy: 0.5720 - f1-score: 0.7278\n",
            "<7>        tp: 823 - fp: 330 - fn: 438 - tn: 13746 - precision: 0.7138 - recall: 0.6527 - accuracy: 0.5173 - f1-score: 0.6819\n",
            "<8>        tp: 249 - fp: 87 - fn: 594 - tn: 14407 - precision: 0.7411 - recall: 0.2954 - accuracy: 0.2677 - f1-score: 0.4224\n",
            "<9>        tp: 353 - fp: 34 - fn: 99 - tn: 14851 - precision: 0.9121 - recall: 0.7810 - accuracy: 0.7263 - f1-score: 0.8415\n",
            "2019-07-10 02:28:52,276 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlTGwMVuMsz9",
        "colab_type": "code",
        "outputId": "a8debf12-226a-48a7-9fd9-4eb441f431ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "word_embeddings = [ELMoEmbeddings('pubmed')]\n",
        "flair_model(word_embeddings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  DeprecationWarning)\n",
            "100%|██████████| 336/336 [00:00<00:00, 84825.22B/s]\n",
            "100%|██████████| 374434792/374434792 [00:09<00:00, 39818727.70B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MODEL METADATA:\n",
            "    hidden_size=512 | learning_rate=0.1 | max_epochs=5\n",
            "\n",
            "\n",
            "2019-07-10 02:29:21,139 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 02:29:21,140 Evaluation method: MICRO_F1_SCORE\n",
            "2019-07-10 02:29:21,596 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 02:29:22,425 epoch 1 - iter 0/52697 - loss 2.53972983\n",
            "2019-07-10 02:37:46,702 epoch 1 - iter 5269/52697 - loss 1.16806028\n",
            "2019-07-10 02:46:08,859 epoch 1 - iter 10538/52697 - loss 1.11165910\n",
            "2019-07-10 02:54:30,411 epoch 1 - iter 15807/52697 - loss 1.09069902\n",
            "2019-07-10 03:02:53,373 epoch 1 - iter 21076/52697 - loss 1.07789306\n",
            "2019-07-10 03:11:14,741 epoch 1 - iter 26345/52697 - loss 1.06892765\n",
            "2019-07-10 03:19:37,729 epoch 1 - iter 31614/52697 - loss 1.06106354\n",
            "2019-07-10 03:27:59,663 epoch 1 - iter 36883/52697 - loss 1.05375273\n",
            "2019-07-10 03:36:22,962 epoch 1 - iter 42152/52697 - loss 1.04840894\n",
            "2019-07-10 03:44:45,677 epoch 1 - iter 47421/52697 - loss 1.04347402\n",
            "2019-07-10 03:53:06,393 epoch 1 - iter 52690/52697 - loss 1.03922249\n",
            "2019-07-10 03:53:07,184 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 03:53:07,186 EPOCH 1 done: loss 1.0392 - lr 0.1000 - bad epochs 0\n",
            "2019-07-10 03:54:03,749 DEV : loss 0.9870067238807678 - score 0.6986\n",
            "2019-07-10 03:54:59,872 TEST : loss 0.9870083928108215 - score 0.6986\n",
            "2019-07-10 03:55:00,863 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 03:55:01,823 epoch 2 - iter 0/52697 - loss 1.26952493\n",
            "2019-07-10 04:03:28,675 epoch 2 - iter 5269/52697 - loss 0.98973302\n",
            "2019-07-10 04:11:53,017 epoch 2 - iter 10538/52697 - loss 0.99377996\n",
            "2019-07-10 04:20:19,602 epoch 2 - iter 15807/52697 - loss 0.99240421\n",
            "2019-07-10 04:28:40,937 epoch 2 - iter 21076/52697 - loss 0.99187430\n",
            "2019-07-10 04:37:05,850 epoch 2 - iter 26345/52697 - loss 0.99167940\n",
            "2019-07-10 04:45:28,363 epoch 2 - iter 31614/52697 - loss 0.99026364\n",
            "2019-07-10 04:53:52,928 epoch 2 - iter 36883/52697 - loss 0.98879556\n",
            "2019-07-10 05:02:16,131 epoch 2 - iter 42152/52697 - loss 0.98822044\n",
            "2019-07-10 05:10:38,547 epoch 2 - iter 47421/52697 - loss 0.98736217\n",
            "2019-07-10 05:19:02,233 epoch 2 - iter 52690/52697 - loss 0.98650897\n",
            "2019-07-10 05:19:03,086 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 05:19:03,087 EPOCH 2 done: loss 0.9865 - lr 0.1000 - bad epochs 0\n",
            "2019-07-10 05:19:59,942 DEV : loss 1.013389229774475 - score 0.6951\n",
            "2019-07-10 05:20:56,299 TEST : loss 1.0133832693099976 - score 0.6951\n",
            "2019-07-10 05:20:56,300 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 05:20:57,144 epoch 3 - iter 0/52697 - loss 0.81439829\n",
            "2019-07-10 05:29:27,127 epoch 3 - iter 5269/52697 - loss 0.97449315\n",
            "2019-07-10 05:37:52,553 epoch 3 - iter 10538/52697 - loss 0.97481330\n",
            "2019-07-10 05:46:15,847 epoch 3 - iter 15807/52697 - loss 0.97541424\n",
            "2019-07-10 05:54:39,872 epoch 3 - iter 21076/52697 - loss 0.97405352\n",
            "2019-07-10 06:03:06,100 epoch 3 - iter 26345/52697 - loss 0.97252779\n",
            "2019-07-10 06:11:33,475 epoch 3 - iter 31614/52697 - loss 0.97277320\n",
            "2019-07-10 06:20:04,662 epoch 3 - iter 36883/52697 - loss 0.97229673\n",
            "2019-07-10 06:28:34,387 epoch 3 - iter 42152/52697 - loss 0.97159148\n",
            "2019-07-10 06:37:03,028 epoch 3 - iter 47421/52697 - loss 0.97146243\n",
            "2019-07-10 06:45:33,655 epoch 3 - iter 52690/52697 - loss 0.97130220\n",
            "2019-07-10 06:45:34,617 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 06:45:34,619 EPOCH 3 done: loss 0.9713 - lr 0.1000 - bad epochs 1\n",
            "2019-07-10 06:46:31,557 DEV : loss 0.9662286639213562 - score 0.7061\n",
            "2019-07-10 06:47:28,889 TEST : loss 0.9662312269210815 - score 0.7061\n",
            "2019-07-10 06:47:29,949 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 06:47:30,900 epoch 4 - iter 0/52697 - loss 1.26583421\n",
            "2019-07-10 06:56:01,761 epoch 4 - iter 5269/52697 - loss 0.96105797\n",
            "2019-07-10 07:04:33,335 epoch 4 - iter 10538/52697 - loss 0.96478269\n",
            "2019-07-10 07:13:05,792 epoch 4 - iter 15807/52697 - loss 0.96465934\n",
            "2019-07-10 07:21:36,408 epoch 4 - iter 21076/52697 - loss 0.96456040\n",
            "2019-07-10 07:30:02,552 epoch 4 - iter 26345/52697 - loss 0.96405583\n",
            "2019-07-10 07:38:25,834 epoch 4 - iter 31614/52697 - loss 0.96363632\n",
            "2019-07-10 07:46:49,931 epoch 4 - iter 36883/52697 - loss 0.96289467\n",
            "2019-07-10 07:55:14,186 epoch 4 - iter 42152/52697 - loss 0.96312644\n",
            "2019-07-10 08:03:37,893 epoch 4 - iter 47421/52697 - loss 0.96257047\n",
            "2019-07-10 08:12:01,137 epoch 4 - iter 52690/52697 - loss 0.96310331\n",
            "2019-07-10 08:12:02,031 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 08:12:02,036 EPOCH 4 done: loss 0.9631 - lr 0.1000 - bad epochs 0\n",
            "2019-07-10 08:12:58,272 DEV : loss 0.9591545462608337 - score 0.7093\n",
            "2019-07-10 08:13:54,854 TEST : loss 0.9591509103775024 - score 0.7093\n",
            "2019-07-10 08:13:55,813 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 08:13:56,885 epoch 5 - iter 0/52697 - loss 1.01314414\n",
            "2019-07-10 08:22:21,368 epoch 5 - iter 5269/52697 - loss 0.95394805\n",
            "2019-07-10 08:30:44,480 epoch 5 - iter 10538/52697 - loss 0.95833945\n",
            "2019-07-10 08:39:08,879 epoch 5 - iter 15807/52697 - loss 0.95683936\n",
            "2019-07-10 08:47:30,785 epoch 5 - iter 21076/52697 - loss 0.95811489\n",
            "2019-07-10 08:55:54,811 epoch 5 - iter 26345/52697 - loss 0.95681295\n",
            "2019-07-10 09:04:18,377 epoch 5 - iter 31614/52697 - loss 0.95640978\n",
            "2019-07-10 09:12:43,584 epoch 5 - iter 36883/52697 - loss 0.95718927\n",
            "2019-07-10 09:21:13,662 epoch 5 - iter 42152/52697 - loss 0.95692918\n",
            "2019-07-10 09:29:47,832 epoch 5 - iter 47421/52697 - loss 0.95682265\n",
            "2019-07-10 09:38:20,571 epoch 5 - iter 52690/52697 - loss 0.95707365\n",
            "2019-07-10 09:38:21,394 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 09:38:21,397 EPOCH 5 done: loss 0.9571 - lr 0.1000 - bad epochs 0\n",
            "2019-07-10 09:39:18,541 DEV : loss 0.9432886838912964 - score 0.7095\n",
            "2019-07-10 09:40:16,975 TEST : loss 0.9432786703109741 - score 0.7095\n",
            "2019-07-10 09:40:19,505 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 09:40:19,506 Testing using best model ...\n",
            "2019-07-10 09:40:19,508 loading file flair/best-model.pt\n",
            "2019-07-10 09:41:16,633 0.7095\t0.7095\t0.7095\n",
            "2019-07-10 09:41:16,634 \n",
            "MICRO_AVG: acc 0.5497 - f1-score 0.7095\n",
            "MACRO_AVG: acc 0.5542 - f1-score 0.7068615384615385\n",
            "<0>        tp: 564 - fp: 333 - fn: 315 - tn: 14125 - precision: 0.6288 - recall: 0.6416 - accuracy: 0.4653 - f1-score: 0.6351\n",
            "<10>       tp: 457 - fp: 73 - fn: 185 - tn: 14622 - precision: 0.8623 - recall: 0.7118 - accuracy: 0.6392 - f1-score: 0.7799\n",
            "<11>       tp: 1462 - fp: 490 - fn: 474 - tn: 12911 - precision: 0.7490 - recall: 0.7552 - accuracy: 0.6026 - f1-score: 0.7521\n",
            "<12>       tp: 283 - fp: 97 - fn: 111 - tn: 14846 - precision: 0.7447 - recall: 0.7183 - accuracy: 0.5764 - f1-score: 0.7313\n",
            "<1>        tp: 2374 - fp: 1081 - fn: 477 - tn: 11405 - precision: 0.6871 - recall: 0.8327 - accuracy: 0.6038 - f1-score: 0.7529\n",
            "<2>        tp: 295 - fp: 91 - fn: 144 - tn: 14807 - precision: 0.7642 - recall: 0.6720 - accuracy: 0.5566 - f1-score: 0.7151\n",
            "<3>        tp: 1270 - fp: 792 - fn: 626 - tn: 12649 - precision: 0.6159 - recall: 0.6698 - accuracy: 0.4725 - f1-score: 0.6417\n",
            "<4>        tp: 863 - fp: 535 - fn: 285 - tn: 13654 - precision: 0.6173 - recall: 0.7517 - accuracy: 0.5128 - f1-score: 0.6779\n",
            "<5>        tp: 676 - fp: 164 - fn: 306 - tn: 14191 - precision: 0.8048 - recall: 0.6884 - accuracy: 0.5899 - f1-score: 0.7421\n",
            "<6>        tp: 1167 - fp: 421 - fn: 447 - tn: 13302 - precision: 0.7349 - recall: 0.7230 - accuracy: 0.5735 - f1-score: 0.7289\n",
            "<7>        tp: 816 - fp: 264 - fn: 445 - tn: 13812 - precision: 0.7556 - recall: 0.6471 - accuracy: 0.5351 - f1-score: 0.6972\n",
            "<8>        tp: 275 - fp: 81 - fn: 568 - tn: 14413 - precision: 0.7725 - recall: 0.3262 - accuracy: 0.2976 - f1-score: 0.4587\n",
            "<9>        tp: 379 - fp: 34 - fn: 73 - tn: 14851 - precision: 0.9177 - recall: 0.8385 - accuracy: 0.7798 - f1-score: 0.8763\n",
            "2019-07-10 09:41:16,638 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbUZncQDPFGr",
        "colab_type": "code",
        "outputId": "5d0d349d-1af4-4b65-b7e8-13dbbaca1a15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "word_embeddings = [ELMoEmbeddings('original')]\n",
        "flair_model(word_embeddings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 336/336 [00:00<00:00, 82087.96B/s]\n",
            "100%|██████████| 374434792/374434792 [00:09<00:00, 40326719.09B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MODEL METADATA:\n",
            "    hidden_size=512 | learning_rate=0.1 | max_epochs=5\n",
            "\n",
            "\n",
            "2019-07-10 09:41:40,652 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 09:41:40,653 Evaluation method: MICRO_F1_SCORE\n",
            "2019-07-10 09:41:41,070 ----------------------------------------------------------------------------------------------------\n",
            "2019-07-10 09:41:41,975 epoch 1 - iter 0/52697 - loss 2.76070738\n",
            "2019-07-10 09:50:12,604 epoch 1 - iter 5269/52697 - loss 1.46818150\n",
            "2019-07-10 09:58:37,828 epoch 1 - iter 10538/52697 - loss 1.37977914\n",
            "2019-07-10 10:07:02,311 epoch 1 - iter 15807/52697 - loss 1.34140393\n",
            "2019-07-10 10:15:29,523 epoch 1 - iter 21076/52697 - loss 1.31820293\n",
            "2019-07-10 10:23:53,804 epoch 1 - iter 26345/52697 - loss 1.30037777\n",
            "2019-07-10 10:32:19,894 epoch 1 - iter 31614/52697 - loss 1.28788041\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}